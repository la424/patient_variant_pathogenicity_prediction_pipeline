{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHD Variant Structural Analysis Pipeline (v5.2)\n",
    "\n",
    "**Changes from v5.1:**\n",
    "- **DDG concordance bug fix**: DDG vote now uses `max(|mono|, |multi_max|, |multi_min|)` \u2014 v5.1 only checked multi_max for positive thresholds and multi_min for negative, missing single-partner multimer values stored in multi_max that were negative (affects ZIC3 H318N, K405E, R350G, S402P)\n",
    "- **DDG confidence gating uses `ddg_confidence` column**: cleaner than raw pLDDT thresholds, same result\n",
    "- **New sub-score columns**: `structure_strict`, `structure_relaxed`, `external_strict`, `external_relaxed` (and T3 variants) for transparent vote decomposition\n",
    "- **Column ordering**: concordance columns grouped logically with sub-scores before totals\n",
    "\n",
    "**Concordance formula (v5.2):**\n",
    "```\n",
    "DDG value = max( |ddg_monomer|, |ddg_multimer_max|, |ddg_multimer_min| )\n",
    "\n",
    "Standard:  DDG vote = 1 if value >= 2.0 AND ddg_confidence = 'high'\n",
    "Relaxed:   DDG vote = 1 if value >= 1.0 AND ddg_confidence != 'low'\n",
    "\n",
    "AM strict:  likely_pathogenic\n",
    "AM relaxed: likely_pathogenic OR ambiguous\n",
    "\n",
    "Franklin strict:  pathogenic, LP, VUS(high)\n",
    "Franklin relaxed: pathogenic, LP, VUS(high), VUS(mid)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 1: CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "import os, sys, re, warnings, subprocess, tempfile, shutil\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional, Set\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from Bio.PDB import PDBParser, MMCIFParser, NeighborSearch, ShrakeRupley\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# =============================================================================\n",
    "# PATHS \u2014 adjust these to your local environment\n",
    "# =============================================================================\n",
    "# Set WORKING_DIR to the root of your project (where structure files live)\n",
    "# Default: current directory\n",
    "WORKING_DIR    = Path(os.environ.get(\"CHD_WORKING_DIR\", \".\")).resolve()\n",
    "RESULTS_DIR    = WORKING_DIR / \"results\"\n",
    "\n",
    "# mkdssp binary \u2014 install via: conda install -c salilab dssp  OR  brew install dssp\n",
    "DSSP_PATH      = os.environ.get(\"DSSP_PATH\", shutil.which(\"mkdssp\") or \"mkdssp\")\n",
    "\n",
    "for d in [RESULTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Directories to search for structure files (in priority order)\n",
    "SEARCH_DIRS = [\n",
    "    WORKING_DIR,\n",
    "    WORKING_DIR / \"structures\",\n",
    "    WORKING_DIR / \"structures\" / \"monomers\",\n",
    "    WORKING_DIR / \"structures\" / \"multimers\",\n",
    "]\n",
    "\n",
    "def find_file(filename):\n",
    "    \"\"\"Search multiple directories for a file.\"\"\"\n",
    "    if filename is None:\n",
    "        return None\n",
    "    for d in SEARCH_DIRS:\n",
    "        p = d / filename\n",
    "        if p.exists():\n",
    "            return p\n",
    "    return None\n",
    "\n",
    "# =============================================================================\n",
    "# INPUT FILES\n",
    "# =============================================================================\n",
    "VARIANTS_FILE    = WORKING_DIR / \"variants_with_alphamissense_and_franklin_expanded.csv\"\n",
    "MONOMER_DDG_FILE = WORKING_DIR / \"foldx_ddg_monomer_results_all.csv\"\n",
    "MULTIMER_DDG_FILE = WORKING_DIR / \"foldx_ddg_multimer_results.csv\"\n",
    "\n",
    "# =============================================================================\n",
    "# MONOMER STRUCTURE DEFINITIONS\n",
    "# gene \u2192 (cif_filename_or_None, pdb_filename_or_None)\n",
    "# CIF preferred for pLDDT on shroom3/cdh2/dvl2/ctnnb1/zic3/actb (PDB B-factors=0)\n",
    "# PDB B-factors valid for gli3/kpna1/kpna6/mdfi/rock2/tcf7l1\n",
    "# =============================================================================\n",
    "MONOMER_STRUCTURES = {\n",
    "    'shroom3': ('fold_shroom3_model_0.cif',  'fold_shroom3_model_0.pdb'),\n",
    "    'zic3':    ('fold_zic3_model_0.cif',     'fold_zic3_model_0.pdb'),\n",
    "    'cdh2':    ('fold_cdh2_model_0.cif',     'fold_cdh2_model_0.pdb'),\n",
    "    'dvl2':    ('fold_dvl2_model_0.cif',     'fold_dvl2_model_0.pdb'),\n",
    "    'ctnnb1':  ('fold_ctnnb1_model_0.cif',   'fold_ctnnb1_model_0.pdb'),\n",
    "    'actb':    ('fold_actb_model_0.cif',     'fold_actb_model_0.pdb'),\n",
    "    'rock2':   (None,                        'rock2.pdb'),\n",
    "    'gli3':    (None,                        'gli3.pdb'),\n",
    "    'kpna1':   (None,                        'kpna1.pdb'),\n",
    "    'kpna6':   (None,                        'kpna6.pdb'),\n",
    "    'mdfi':    (None,                        'mdfi.pdb'),\n",
    "    'tcf7l1':  (None,                        'tcf7l1.pdb'),\n",
    "    'zic2':    (None,                        'zic2.pdb'),\n",
    "    'zic5':    (None,                        'zic5.pdb'),\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MULTIMER COMPLEX DEFINITIONS\n",
    "# (gene1, partner_label, cif_file_or_None, pdb_file, chain_gene1, chain_partner, is_primary)\n",
    "# NOTE: For pLDDT, PDB B-factors are used (not CIF) because CIF chain order\n",
    "# varies by complex (e.g., actb CIF has chains swapped vs PDB).\n",
    "# =============================================================================\n",
    "MULTIMER_STRUCTURES = [\n",
    "    ('shroom3','actin',         'fold_shroom3_actin_chain_model_0.cif',            'fold_shroom3_actin_chain_model_0.pdb',        'A','B', True),\n",
    "    ('shroom3','actb',          'fold_shroom3_actb_model_0.cif',                   'fold_shroom3_actb_model_0.pdb',               'B','A', False),\n",
    "    ('shroom3','dvl2',          'fold_shroom3_dvl2_model_0.cif',                   'fold_shroom3_dvl2_model_0.pdb',               'A','B', True),\n",
    "    ('shroom3','cdh2_truncated','fold_shroom3_cdh2_truncated_model_0.cif',         'fold_shroom3_cdh2_truncated_model_0.pdb',     'A','B', False),\n",
    "    ('shroom3','ctnnb1',        'fold_shroom3_ctnnb1_model_0.cif',                 'fold_shroom3_ctnnb1_model_0.pdb',             'A','B', True),\n",
    "    ('shroom3','cdh2_cyto',     'fold_shroom3_cdh2_cytoplasmic_domain_model_0.cif','fold_shroom3_cdh2_cytoplasmic_domain.pdb',    'A','B', True),\n",
    "    ('shroom3','actb_no_bind',  'fold_shroom3_no_actin_binding_actb_chain_model_0.cif','fold_shroom3_no_actin_binding_actb_chain.pdb','A','B', False),\n",
    "    ('shroom3','rock2',         None,                                              'fold_shroom3_rock2_model_0.pdb',              'A','B', True),\n",
    "    ('zic3','gli3',   'fold_zic3_gli3_model_0.cif',   'fold_zic3_gli3_model_0.pdb',  'A','B', True),\n",
    "    ('zic3','kpna1',  'fold_zic3_kpna1_model_0.cif',  'fold_zic3_kpna1_model_0.pdb', 'A','B', True),\n",
    "    ('zic3','kpna6',  'fold_zic3_kpna6_model_0.cif',  'fold_zic3_kpna6_model_0.pdb', 'A','B', True),\n",
    "    ('zic3','mdfi',   'fold_zic3_mdfi_model_0.cif',   'fold_zic3_mdfi_model_0.pdb',  'A','B', True),\n",
    "    ('zic3','tcf7l1', 'fold_zic3_tcf7l1_model_0.cif', 'fold_zic3_tcf7l1_model_0.pdb','A','B', True),\n",
    "]\n",
    "\n",
    "# =============================================================================\n",
    "# AMINO ACID DATA\n",
    "# =============================================================================\n",
    "THREE_TO_ONE = {\n",
    "    'ALA':'A','CYS':'C','ASP':'D','GLU':'E','PHE':'F','GLY':'G','HIS':'H',\n",
    "    'ILE':'I','LYS':'K','LEU':'L','MET':'M','ASN':'N','PRO':'P','GLN':'Q',\n",
    "    'ARG':'R','SER':'S','THR':'T','VAL':'V','TRP':'W','TYR':'Y',\n",
    "}\n",
    "AA_PROPERTIES = {\n",
    "    'A':{'size':'small','charge':'neutral','hydrophobic':True},\n",
    "    'R':{'size':'large','charge':'positive','hydrophobic':False},\n",
    "    'N':{'size':'medium','charge':'neutral','hydrophobic':False},\n",
    "    'D':{'size':'medium','charge':'negative','hydrophobic':False},\n",
    "    'C':{'size':'small','charge':'neutral','hydrophobic':True},\n",
    "    'E':{'size':'medium','charge':'negative','hydrophobic':False},\n",
    "    'Q':{'size':'medium','charge':'neutral','hydrophobic':False},\n",
    "    'G':{'size':'small','charge':'neutral','hydrophobic':False},\n",
    "    'H':{'size':'medium','charge':'positive','hydrophobic':False},\n",
    "    'I':{'size':'medium','charge':'neutral','hydrophobic':True},\n",
    "    'L':{'size':'medium','charge':'neutral','hydrophobic':True},\n",
    "    'K':{'size':'large','charge':'positive','hydrophobic':False},\n",
    "    'M':{'size':'medium','charge':'neutral','hydrophobic':True},\n",
    "    'F':{'size':'large','charge':'neutral','hydrophobic':True},\n",
    "    'P':{'size':'small','charge':'neutral','hydrophobic':False},\n",
    "    'S':{'size':'small','charge':'neutral','hydrophobic':False},\n",
    "    'T':{'size':'small','charge':'neutral','hydrophobic':False},\n",
    "    'W':{'size':'large','charge':'neutral','hydrophobic':True},\n",
    "    'Y':{'size':'large','charge':'neutral','hydrophobic':False},\n",
    "    'V':{'size':'small','charge':'neutral','hydrophobic':True},\n",
    "}\n",
    "# Max SASA (Tien et al 2013, theoretical Gly-X-Gly)\n",
    "MAX_SASA = {\n",
    "    'A':129,'R':274,'N':195,'D':193,'C':167,'E':223,'Q':225,'G':104,\n",
    "    'H':224,'I':197,'L':201,'K':236,'M':224,'F':240,'P':159,'S':155,\n",
    "    'T':172,'V':174,'W':285,'Y':263,\n",
    "}\n",
    "\n",
    "GRANTHAM = {\n",
    "    ('A','R'):112,('A','N'):111,('A','D'):126,('A','C'):195,('A','Q'):91,('A','E'):107,\n",
    "    ('A','G'):60,('A','H'):86,('A','I'):94,('A','L'):96,('A','K'):106,('A','M'):84,\n",
    "    ('A','F'):113,('A','P'):27,('A','S'):99,('A','T'):58,('A','W'):148,('A','Y'):112,('A','V'):64,\n",
    "    ('R','N'):86,('R','D'):96,('R','C'):180,('R','Q'):43,('R','E'):54,('R','G'):125,\n",
    "    ('R','H'):29,('R','I'):97,('R','L'):102,('R','K'):26,('R','M'):91,('R','F'):97,\n",
    "    ('R','P'):103,('R','S'):110,('R','T'):71,('R','W'):101,('R','Y'):77,('R','V'):96,\n",
    "    ('N','D'):23,('N','C'):139,('N','Q'):46,('N','E'):42,('N','G'):80,('N','H'):68,\n",
    "    ('N','I'):149,('N','L'):153,('N','K'):94,('N','M'):142,('N','F'):158,('N','P'):91,\n",
    "    ('N','S'):46,('N','T'):65,('N','W'):174,('N','Y'):143,('N','V'):133,\n",
    "    ('D','C'):154,('D','Q'):61,('D','E'):45,('D','G'):94,('D','H'):81,('D','I'):168,\n",
    "    ('D','L'):172,('D','K'):101,('D','M'):160,('D','F'):177,('D','P'):108,('D','S'):65,\n",
    "    ('D','T'):85,('D','W'):181,('D','Y'):160,('D','V'):152,\n",
    "    ('C','Q'):154,('C','E'):170,('C','G'):159,('C','H'):174,('C','I'):198,('C','L'):198,\n",
    "    ('C','K'):202,('C','M'):196,('C','F'):205,('C','P'):169,('C','S'):112,('C','T'):149,\n",
    "    ('C','W'):215,('C','Y'):194,('C','V'):192,\n",
    "    ('Q','E'):29,('Q','G'):87,('Q','H'):24,('Q','I'):109,('Q','L'):113,('Q','K'):53,\n",
    "    ('Q','M'):101,('Q','F'):116,('Q','P'):76,('Q','S'):68,('Q','T'):42,('Q','W'):130,\n",
    "    ('Q','Y'):99,('Q','V'):96,\n",
    "    ('E','G'):98,('E','H'):40,('E','I'):134,('E','L'):138,('E','K'):56,('E','M'):126,\n",
    "    ('E','F'):140,('E','P'):93,('E','S'):80,('E','T'):65,('E','W'):152,('E','Y'):122,('E','V'):121,\n",
    "    ('G','H'):98,('G','I'):135,('G','L'):138,('G','K'):127,('G','M'):127,('G','F'):153,\n",
    "    ('G','P'):42,('G','S'):56,('G','T'):59,('G','W'):184,('G','Y'):147,('G','V'):109,\n",
    "    ('H','I'):94,('H','L'):99,('H','K'):32,('H','M'):87,('H','F'):100,('H','P'):77,\n",
    "    ('H','S'):89,('H','T'):47,('H','W'):115,('H','Y'):83,('H','V'):84,\n",
    "    ('I','L'):5,('I','K'):102,('I','M'):10,('I','F'):21,('I','P'):95,('I','S'):142,\n",
    "    ('I','T'):89,('I','W'):61,('I','Y'):33,('I','V'):29,\n",
    "    ('L','K'):107,('L','M'):15,('L','F'):22,('L','P'):98,('L','S'):145,('L','T'):92,\n",
    "    ('L','W'):61,('L','Y'):36,('L','V'):32,\n",
    "    ('K','M'):95,('K','F'):102,('K','P'):103,('K','S'):121,('K','T'):78,('K','W'):110,\n",
    "    ('K','Y'):85,('K','V'):97,\n",
    "    ('M','F'):28,('M','P'):87,('M','S'):135,('M','T'):81,('M','W'):67,('M','Y'):36,('M','V'):21,\n",
    "    ('F','P'):114,('F','S'):155,('F','T'):103,('F','W'):40,('F','Y'):22,('F','V'):50,\n",
    "    ('P','S'):74,('P','T'):38,('P','W'):147,('P','Y'):110,('P','V'):68,\n",
    "    ('S','T'):58,('S','W'):177,('S','Y'):144,('S','V'):124,\n",
    "    ('T','W'):128,('T','Y'):92,('T','V'):69,\n",
    "    ('W','Y'):37,('W','V'):88,\n",
    "    ('Y','V'):55,\n",
    "}\n",
    "\n",
    "def get_grantham(a1, a2):\n",
    "    if pd.isna(a1) or pd.isna(a2): return -1\n",
    "    a1, a2 = str(a1).upper(), str(a2).upper()\n",
    "    if a1 == a2: return 0\n",
    "    return GRANTHAM.get((a1,a2), GRANTHAM.get((a2,a1), -1))\n",
    "\n",
    "def classify_grantham(d):\n",
    "    if pd.isna(d) or d is None or d < 0: return 'unknown'\n",
    "    d = int(d)\n",
    "    if d <= 50: return 'conservative'\n",
    "    elif d <= 100: return 'moderately_conservative'\n",
    "    elif d <= 150: return 'moderately_radical'\n",
    "    else: return 'radical'\n",
    "\n",
    "def grantham_severity(d):\n",
    "    if pd.isna(d) or d is None or d < 0: return 0.0\n",
    "    return min(4.0, float(d) / 53.75)\n",
    "\n",
    "def get_property_changes(r, a):\n",
    "    if pd.isna(r) or pd.isna(a): return 'unknown'\n",
    "    p1, p2 = AA_PROPERTIES.get(str(r).upper(),{}), AA_PROPERTIES.get(str(a).upper(),{})\n",
    "    if not p1 or not p2: return 'unknown'\n",
    "    ch = []\n",
    "    for k in ['size','charge','hydrophobic']:\n",
    "        if p1.get(k) != p2.get(k): ch.append(f\"{k}:{p1[k]}->{p2[k]}\")\n",
    "    return ';'.join(ch) if ch else 'none'\n",
    "\n",
    "def sf(v, d=0.0):\n",
    "    if pd.isna(v) or v is None: return d\n",
    "    try: return float(v)\n",
    "    except: return d\n",
    "def si(v, d=0):\n",
    "    if pd.isna(v) or v is None: return d\n",
    "    try: return int(v)\n",
    "    except: return d\n",
    "def ss(v): return '' if pd.isna(v) or v is None else str(v)\n",
    "def sb(v, d=False):\n",
    "    if pd.isna(v) or v is None: return d\n",
    "    return bool(v)\n",
    "\n",
    "# Verify files exist\n",
    "found = 0\n",
    "for g, (cif, pdb) in MONOMER_STRUCTURES.items():\n",
    "    cf = find_file(cif)\n",
    "    pf = find_file(pdb)\n",
    "    if cf or pf:\n",
    "        found += 1\n",
    "    else:\n",
    "        print(f\"  \u26a0 {g}: no structure found (tried {cif}, {pdb})\")\n",
    "print(f\"\u2713 Configuration loaded: {found}/{len(MONOMER_STRUCTURES)} monomer structures found\")\n",
    "print(f\"  Multimer complexes: {len(MULTIMER_STRUCTURES)}\")\n",
    "\n",
    "mfound = 0\n",
    "for g1, pl, cif, pdb, *_ in MULTIMER_STRUCTURES:\n",
    "    pf = find_file(pdb)\n",
    "    if pf: mfound += 1\n",
    "    else: print(f\"  \u26a0 {g1}-{pl}: {pdb} NOT FOUND\")\n",
    "print(f\"  Multimer PDBs found: {mfound}/{len(MULTIMER_STRUCTURES)}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 2: STRUCTURE LOADING AND EXTRACTION FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "_pdb_parser = PDBParser(QUIET=True)\n",
    "_cif_parser = MMCIFParser(QUIET=True)\n",
    "\n",
    "\n",
    "def load_cif(path):\n",
    "    if path and path.exists():\n",
    "        try: return _cif_parser.get_structure('s', str(path))\n",
    "        except: pass\n",
    "    return None\n",
    "\n",
    "def load_pdb(path):\n",
    "    if path and path.exists():\n",
    "        try: return _pdb_parser.get_structure('s', str(path))\n",
    "        except: pass\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_plddt(structure, chain_id='A'):\n",
    "    \"\"\"Per-residue pLDDT from B-factors. Returns only non-zero values.\"\"\"\n",
    "    plddt = {}\n",
    "    if structure is None: return plddt\n",
    "    model = structure[0]\n",
    "    # Resolve chain\n",
    "    if chain_id not in model:\n",
    "        for c in model: chain_id = c.id; break\n",
    "    if chain_id not in model: return plddt\n",
    "    for res in model[chain_id].get_residues():\n",
    "        if res.id[0] == ' ':\n",
    "            p = None\n",
    "            if 'CA' in res: p = res['CA'].bfactor\n",
    "            else:\n",
    "                for atom in res: p = atom.bfactor; break\n",
    "            if p is not None and p > 0:\n",
    "                plddt[res.id[1]] = round(p, 2)\n",
    "    return plddt\n",
    "\n",
    "\n",
    "def get_monomer_plddt(gene_lower):\n",
    "    \"\"\"Get pLDDT for monomer: CIF first (needed for shroom3/cdh2/dvl2/ctnnb1/zic3/actb),\n",
    "    then PDB fallback.\"\"\"\n",
    "    cif_name, pdb_name = MONOMER_STRUCTURES.get(gene_lower, (None, None))\n",
    "    cif_path = find_file(cif_name)\n",
    "    pdb_path = find_file(pdb_name)\n",
    "\n",
    "    # Try CIF first\n",
    "    if cif_path:\n",
    "        struct = load_cif(cif_path)\n",
    "        if struct:\n",
    "            plddt = get_plddt(struct, 'A')\n",
    "            if plddt:\n",
    "                return plddt, struct, 'cif', cif_path\n",
    "    # PDB fallback\n",
    "    if pdb_path:\n",
    "        struct = load_pdb(pdb_path)\n",
    "        if struct:\n",
    "            plddt = get_plddt(struct, 'A')\n",
    "            if plddt:\n",
    "                return plddt, struct, 'pdb', pdb_path\n",
    "            # Even if pLDDT empty, return struct for contacts\n",
    "            return {}, struct, 'pdb_no_plddt', pdb_path\n",
    "    return {}, None, None, None\n",
    "\n",
    "\n",
    "def get_multimer_plddt(pdb_path, cif_path, chain_id):\n",
    "    \"\"\"Get pLDDT for multimer: PDB first (consistent chain order), CIF fallback.\n",
    "    AlphaFold multimer PDBs should have valid B-factors.\"\"\"\n",
    "    # PDB first \u2014 chain order is consistent\n",
    "    struct = load_pdb(pdb_path)\n",
    "    if struct:\n",
    "        plddt = get_plddt(struct, chain_id)\n",
    "        if plddt:\n",
    "            return plddt, 'pdb'\n",
    "    # CIF fallback (WARNING: chain order may differ!)\n",
    "    struct = load_cif(cif_path)\n",
    "    if struct:\n",
    "        plddt = get_plddt(struct, chain_id)\n",
    "        if plddt:\n",
    "            return plddt, 'cif'\n",
    "    return {}, None\n",
    "\n",
    "\n",
    "def get_residue_aa(structure, chain_id='A'):\n",
    "    if structure is None: return {}\n",
    "    model = structure[0]\n",
    "    if chain_id not in model:\n",
    "        for c in model: chain_id = c.id; break\n",
    "    if chain_id not in model: return {}\n",
    "    return {r.id[1]: THREE_TO_ONE.get(r.resname, '?')\n",
    "            for r in model[chain_id].get_residues() if r.id[0] == ' '}\n",
    "\n",
    "\n",
    "def count_contacts(structure, chain_id='A', distance=5.0):\n",
    "    \"\"\"Unique residue-residue contacts, sequence separation >= 3.\n",
    "    Matches original working pipeline (cell 24 of 85-cell notebook).\"\"\"\n",
    "    if structure is None: return {}\n",
    "    model = structure[0]\n",
    "    if chain_id not in model:\n",
    "        for c in model: chain_id = c.id; break\n",
    "    if chain_id not in model: return {}\n",
    "\n",
    "    chain = model[chain_id]\n",
    "    residues = [r for r in chain.get_residues() if r.id[0] == ' ']\n",
    "    contacts = {}\n",
    "\n",
    "    for i, res in enumerate(residues):\n",
    "        pos = res.id[1]\n",
    "        neighbor_set = set()\n",
    "        for j, other in enumerate(residues):\n",
    "            if abs(i - j) < 3:  # skip self and immediate neighbors\n",
    "                continue\n",
    "            for atom_i in res.get_atoms():\n",
    "                found = False\n",
    "                for atom_j in other.get_atoms():\n",
    "                    if atom_i - atom_j < distance:\n",
    "                        neighbor_set.add(other.id[1])\n",
    "                        found = True\n",
    "                        break\n",
    "                if found:\n",
    "                    break\n",
    "        contacts[pos] = len(neighbor_set)\n",
    "    return contacts\n",
    "\n",
    "\n",
    "def count_interface(structure, my_chain, partner_chain, distance=5.0):\n",
    "    \"\"\"Inter-chain contacts: unique partner residues within distance for each residue.\"\"\"\n",
    "    if structure is None: return {}, set()\n",
    "    model = structure[0]\n",
    "    if my_chain not in model or partner_chain not in model:\n",
    "        return {}, set()\n",
    "\n",
    "    partner_atoms = list(model[partner_chain].get_atoms())\n",
    "    if not partner_atoms: return {}, set()\n",
    "    ns = NeighborSearch(partner_atoms)\n",
    "\n",
    "    inter, iface = {}, set()\n",
    "    for res in model[my_chain].get_residues():\n",
    "        if res.id[0] != ' ': continue\n",
    "        partner_residues = set()\n",
    "        for atom in res.get_atoms():\n",
    "            for nb in ns.search(atom.coord, distance, 'R'):\n",
    "                if nb.id[0] == ' ':\n",
    "                    partner_residues.add(nb.id[1])\n",
    "        count = len(partner_residues)\n",
    "        if count > 0:\n",
    "            inter[res.id[1]] = count\n",
    "            iface.add(res.id[1])\n",
    "    return inter, iface\n",
    "\n",
    "\n",
    "def get_accessibility(structure, chain_id='A'):\n",
    "    \"\"\"Relative solvent accessibility using ShrakeRupley (no external tools).\n",
    "    Compatible with Biopython 1.86.\"\"\"\n",
    "    acc = {}\n",
    "    if structure is None: return acc\n",
    "    model = structure[0]\n",
    "    if chain_id not in model:\n",
    "        for c in model: chain_id = c.id; break\n",
    "    if chain_id not in model: return acc\n",
    "    try:\n",
    "        sr = ShrakeRupley()\n",
    "        sr.compute(structure[0], level='R')\n",
    "        for res in model[chain_id].get_residues():\n",
    "            if res.id[0] != ' ': continue\n",
    "            aa = THREE_TO_ONE.get(res.resname, 'X')\n",
    "            max_s = MAX_SASA.get(aa, 200)\n",
    "            rel = min(1.0, res.sasa / max_s) if max_s > 0 and hasattr(res, 'sasa') else None\n",
    "            if rel is not None:\n",
    "                acc[res.id[1]] = round(rel, 9)\n",
    "    except Exception as e:\n",
    "        print(f\"    ShrakeRupley warning: {e}\")\n",
    "    return acc\n",
    "\n",
    "\n",
    "def get_secondary_structure(pdb_path, chain_id='A'):\n",
    "    \"\"\"Secondary structure via mkdssp subprocess (handles v4 output format).\n",
    "    Falls back to empty if mkdssp fails.\"\"\"\n",
    "    ss_map = {}\n",
    "    if pdb_path is None or not pdb_path.exists(): return ss_map\n",
    "    try:\n",
    "        # Try mkdssp v4 with classic DSSP output format\n",
    "        for cmd in [\n",
    "            [DSSP_PATH, '--output-format', 'dssp', str(pdb_path)],\n",
    "            [DSSP_PATH, '-i', str(pdb_path)],\n",
    "            [DSSP_PATH, str(pdb_path)],\n",
    "        ]:\n",
    "            try:\n",
    "                result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)\n",
    "                if result.returncode == 0 and len(result.stdout) > 100:\n",
    "                    break\n",
    "            except FileNotFoundError:\n",
    "                return ss_map\n",
    "        else:\n",
    "            return ss_map\n",
    "\n",
    "        if result.returncode != 0:\n",
    "            return ss_map\n",
    "\n",
    "        # Parse DSSP output\n",
    "        in_data = False\n",
    "        for line in result.stdout.split('\\n'):\n",
    "            if '  #  RESIDUE' in line:\n",
    "                in_data = True\n",
    "                continue\n",
    "            if not in_data or len(line) < 17:\n",
    "                continue\n",
    "            # Skip chain break lines\n",
    "            if line[13] == '!':\n",
    "                continue\n",
    "            try:\n",
    "                chain = line[11]\n",
    "                if chain != chain_id:\n",
    "                    continue\n",
    "                resnum_str = line[5:10].strip()\n",
    "                if not resnum_str:\n",
    "                    continue\n",
    "                resnum = int(resnum_str)\n",
    "                sec = line[16] if len(line) > 16 and line[16] != ' ' else '-'\n",
    "                ss_map[resnum] = sec\n",
    "            except (ValueError, IndexError):\n",
    "                continue\n",
    "    except Exception:\n",
    "        pass\n",
    "    return ss_map\n",
    "\n",
    "\n",
    "def classify_burial(a):\n",
    "    if a is None or pd.isna(a): return 'unknown'\n",
    "    if float(a) < 0.05: return 'buried_core'\n",
    "    elif float(a) < 0.25: return 'partially_buried'\n",
    "    else: return 'surface_exposed'\n",
    "\n",
    "def classify_plddt(v):\n",
    "    if v is None or pd.isna(v): return 'unknown'\n",
    "    if v >= 90: return 'very_high'\n",
    "    elif v >= 70: return 'confident'\n",
    "    elif v >= 50: return 'low'\n",
    "    else: return 'very_low'\n",
    "\n",
    "def classify_contacts(n):\n",
    "    if n is None or pd.isna(n): return 'unknown'\n",
    "    if n >= 8: return 'high_contact'\n",
    "    elif n >= 1: return 'medium_contact'\n",
    "    else: return 'low_contact'\n",
    "\n",
    "print(\"\u2713 Extraction functions defined\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 3: LOAD VARIANTS\n",
    "# =============================================================================\n",
    "\n",
    "variants_df = pd.read_csv(VARIANTS_FILE)\n",
    "variants_df.columns = [c.lower().strip() for c in variants_df.columns]\n",
    "variants_df['position'] = variants_df['position'].astype(int)\n",
    "\n",
    "ann_cols = [c for c in variants_df.columns\n",
    "            if c in ['alphamissense','franklin','alphamissense_pathogenicity']]\n",
    "annotation_df = variants_df[['gene','position','ref_aa','alt_aa'] + ann_cols].copy()\n",
    "\n",
    "print(f\"\u2713 Loaded {len(variants_df)} variants across {variants_df['gene'].nunique()} genes\")\n",
    "print(variants_df.groupby('gene').size().to_string())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 4: MONOMER METRICS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Extracting monomer structural metrics...\")\n",
    "monomer_rows = []\n",
    "\n",
    "for gene in variants_df['gene'].unique():\n",
    "    g = str(gene).lower()\n",
    "    gene_vars = variants_df[variants_df['gene'].str.lower() == g]\n",
    "\n",
    "    # Get pLDDT (CIF first for genes with zero PDB B-factors)\n",
    "    plddt_map, struct_plddt, plddt_src, plddt_path = get_monomer_plddt(g)\n",
    "\n",
    "    # Get PDB structure for contacts and accessibility\n",
    "    _, pdb_name = MONOMER_STRUCTURES.get(g, (None, None))\n",
    "    pdb_path = find_file(pdb_name)\n",
    "    # Also try generic patterns if not found\n",
    "    if pdb_path is None:\n",
    "        for pat in [f'fold_{g}_model_0.pdb', f'{g}.pdb']:\n",
    "            pdb_path = find_file(pat)\n",
    "            if pdb_path: break\n",
    "\n",
    "    struct_pdb = load_pdb(pdb_path)\n",
    "\n",
    "    # Use whichever structure we have for contacts\n",
    "    struct_contacts = struct_pdb or struct_plddt\n",
    "    contact_map = count_contacts(struct_contacts, 'A') if struct_contacts else {}\n",
    "\n",
    "    # Accessibility (ShrakeRupley \u2014 Biopython 1.86 compatible)\n",
    "    acc_map = get_accessibility(struct_pdb or struct_plddt, 'A')\n",
    "\n",
    "    # Secondary structure (subprocess mkdssp \u2014 handles v4 format)\n",
    "    ss_map = get_secondary_structure(pdb_path, 'A')\n",
    "\n",
    "    # AA identity\n",
    "    aa_map = get_residue_aa(struct_plddt or struct_pdb, 'A')\n",
    "\n",
    "    n_plddt = sum(1 for _, r in gene_vars.iterrows() if plddt_map.get(int(r['position'])))\n",
    "    n_acc = sum(1 for _, r in gene_vars.iterrows() if acc_map.get(int(r['position'])) is not None)\n",
    "    has_struct = struct_plddt is not None or struct_pdb is not None\n",
    "    print(f\"  {gene}: struct={'YES' if has_struct else 'NO'} src={plddt_src} pLDDT={n_plddt}/{len(gene_vars)} acc={n_acc}/{len(gene_vars)} contacts={len(contact_map)} SS={len(ss_map)}\")\n",
    "\n",
    "    for _, row in gene_vars.iterrows():\n",
    "        pos = int(row['position'])\n",
    "        gd = get_grantham(row['ref_aa'], row['alt_aa'])\n",
    "        p = plddt_map.get(pos)\n",
    "        c = contact_map.get(pos, 0) if contact_map else None\n",
    "        a = acc_map.get(pos)\n",
    "        sec_struct = ss_map.get(pos, '-') if ss_map else None\n",
    "\n",
    "        monomer_rows.append({\n",
    "            'gene': gene, 'position': pos,\n",
    "            'ref_aa': row['ref_aa'], 'alt_aa': row['alt_aa'],\n",
    "            'grantham_distance': gd, 'grantham_class': classify_grantham(gd),\n",
    "            'substitution_severity': round(grantham_severity(gd), 2),\n",
    "            'property_changes': get_property_changes(row['ref_aa'], row['alt_aa']),\n",
    "            'monomer_plddt': p, 'monomer_plddt_category': classify_plddt(p),\n",
    "            'monomer_n_contacts': float(c) if c is not None else None,\n",
    "            'monomer_contact_category': classify_contacts(c),\n",
    "            'monomer_aa': aa_map.get(pos),\n",
    "            'monomer_accessibility': a,\n",
    "            'monomer_burial': classify_burial(a),\n",
    "            'monomer_secondary_structure': sec_struct,\n",
    "            'monomer_contact_disruption': float(c) if c is not None else None,\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(monomer_rows)\n",
    "print(f\"\\n\u2713 Monomer: {len(df)} variants\")\n",
    "print(f\"  pLDDT populated:     {df['monomer_plddt'].notna().sum()}/{len(df)}\")\n",
    "print(f\"  Accessibility:       {df['monomer_accessibility'].notna().sum()}/{len(df)}\")\n",
    "print(f\"  Burial (non-unknown): {(df['monomer_burial'] != 'unknown').sum()}/{len(df)}\")\n",
    "print(f\"  Secondary structure: {df['monomer_secondary_structure'].notna().sum()}/{len(df)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 5: MULTIMER EXTRACTION (PDB pLDDT + BIDIRECTIONAL)\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Extracting multimer metrics...\")\n",
    "\n",
    "multi_data = {}  # (gene_lower, position) \u2192 {col: val}\n",
    "all_partner_labels = set()\n",
    "variant_genes = set(df['gene'].str.lower().unique())\n",
    "\n",
    "for gene1, partner_label, cif_file, pdb_file, chain1, chain2, is_primary in MULTIMER_STRUCTURES:\n",
    "    g1 = gene1.lower()\n",
    "    plabel = partner_label.lower()\n",
    "\n",
    "    pdb_path = find_file(pdb_file)\n",
    "    cif_path = find_file(cif_file) if cif_file else None\n",
    "\n",
    "    if pdb_path is None:\n",
    "        print(f\"  \u26a0 {pdb_file} NOT FOUND \u2014 skipping {g1}-{plabel}\")\n",
    "        continue\n",
    "\n",
    "    # Load PDB for everything (contacts, interface, accessibility, pLDDT)\n",
    "    struct_pdb = load_pdb(pdb_path)\n",
    "    if struct_pdb is None:\n",
    "        print(f\"  \u26a0 Failed to load {pdb_file}\")\n",
    "        continue\n",
    "\n",
    "    # === FORWARD: gene1 variants get multi_{partner_label}_* ===\n",
    "    if g1 in variant_genes:\n",
    "        all_partner_labels.add(plabel)\n",
    "        gene1_positions = set(df[df['gene'].str.lower() == g1]['position'].values)\n",
    "\n",
    "        # pLDDT from PDB first (consistent chain order), CIF fallback\n",
    "        plddt_a, psrc = get_multimer_plddt(pdb_path, cif_path, chain1)\n",
    "        contacts_a = count_contacts(struct_pdb, chain1)\n",
    "        inter_a, iface_a = count_interface(struct_pdb, chain1, chain2)\n",
    "        acc_a = get_accessibility(struct_pdb, chain1)\n",
    "        ss_a = get_secondary_structure(pdb_path, chain1)\n",
    "\n",
    "        for pos in gene1_positions:\n",
    "            key = (g1, pos)\n",
    "            if key not in multi_data: multi_data[key] = {}\n",
    "            pfx = f\"multi_{plabel}\"\n",
    "            p = plddt_a.get(pos)\n",
    "            c = contacts_a.get(pos, 0)\n",
    "            ic = inter_a.get(pos, 0)\n",
    "            multi_data[key][f\"{pfx}_plddt\"] = p\n",
    "            multi_data[key][f\"{pfx}_n_contacts\"] = float(c)\n",
    "            multi_data[key][f\"{pfx}_inter_contacts\"] = float(ic)\n",
    "            multi_data[key][f\"{pfx}_is_interface\"] = pos in iface_a\n",
    "            multi_data[key][f\"{pfx}_accessibility\"] = acc_a.get(pos)\n",
    "            multi_data[key][f\"{pfx}_burial\"] = classify_burial(acc_a.get(pos))\n",
    "            multi_data[key][f\"{pfx}_sec_struct\"] = ss_a.get(pos, '-')\n",
    "            multi_data[key][f\"{pfx}_disruption\"] = float(c)\n",
    "\n",
    "        n_fwd = len([p for p in gene1_positions if plddt_a.get(p)])\n",
    "        print(f\"  FWD {g1} \u2192 multi_{plabel}: {n_fwd}/{len(gene1_positions)} pLDDT (src={psrc})\")\n",
    "\n",
    "    # === REVERSE: partner gene variants get multi_{gene1}_* ===\n",
    "    partner_gene_map = {\n",
    "        'dvl2':'dvl2','ctnnb1':'ctnnb1','rock2':'rock2',\n",
    "        'gli3':'gli3','kpna1':'kpna1','kpna6':'kpna6',\n",
    "        'mdfi':'mdfi','tcf7l1':'tcf7l1',\n",
    "        'cdh2_truncated':'cdh2','cdh2_cyto':'cdh2',\n",
    "        'actin':'actb','actb':'actb','actb_no_bind':'actb',\n",
    "    }\n",
    "    partner_gene = partner_gene_map.get(plabel, plabel)\n",
    "\n",
    "    if partner_gene in variant_genes:\n",
    "        # Column label: same as forward (multi_{partner_label})\n",
    "        # e.g., rock2 variants in shroom3-rock2 complex \u2192 multi_rock2 (not multi_shroom3)\n",
    "        rev_label = plabel\n",
    "        all_partner_labels.add(rev_label)\n",
    "        partner_positions = set(df[df['gene'].str.lower() == partner_gene]['position'].values)\n",
    "\n",
    "        plddt_b, psrc_b = get_multimer_plddt(pdb_path, cif_path, chain2)\n",
    "        contacts_b = count_contacts(struct_pdb, chain2)\n",
    "        inter_b, iface_b = count_interface(struct_pdb, chain2, chain1)\n",
    "        acc_b = get_accessibility(struct_pdb, chain2)\n",
    "        ss_b = get_secondary_structure(pdb_path, chain2)\n",
    "\n",
    "        for pos in partner_positions:\n",
    "            key = (partner_gene, pos)\n",
    "            if key not in multi_data: multi_data[key] = {}\n",
    "            pfx = f\"multi_{rev_label}\"\n",
    "            # Only write if not already populated (first complex wins)\n",
    "            if f\"{pfx}_plddt\" not in multi_data[key] or multi_data[key][f\"{pfx}_plddt\"] is None:\n",
    "                p = plddt_b.get(pos)\n",
    "                c = contacts_b.get(pos, 0)\n",
    "                ic = inter_b.get(pos, 0)\n",
    "                multi_data[key][f\"{pfx}_plddt\"] = p\n",
    "                multi_data[key][f\"{pfx}_n_contacts\"] = float(c)\n",
    "                multi_data[key][f\"{pfx}_inter_contacts\"] = float(ic)\n",
    "                multi_data[key][f\"{pfx}_is_interface\"] = pos in iface_b\n",
    "                multi_data[key][f\"{pfx}_accessibility\"] = acc_b.get(pos)\n",
    "                multi_data[key][f\"{pfx}_burial\"] = classify_burial(acc_b.get(pos))\n",
    "                multi_data[key][f\"{pfx}_sec_struct\"] = ss_b.get(pos, '-')\n",
    "                multi_data[key][f\"{pfx}_disruption\"] = float(c)\n",
    "\n",
    "        n_rev = len([p for p in partner_positions if plddt_b.get(p)])\n",
    "        print(f\"  REV {partner_gene} \u2192 multi_{rev_label}: {n_rev}/{len(partner_positions)} pLDDT (src={psrc_b})\")\n",
    "\n",
    "# Merge into df\n",
    "multi_df = pd.DataFrame.from_dict(multi_data, orient='index')\n",
    "multi_df.index = pd.MultiIndex.from_tuples(multi_df.index, names=['gene_lower','position'])\n",
    "multi_df = multi_df.reset_index()\n",
    "\n",
    "df['gene_lower'] = df['gene'].str.lower()\n",
    "df = df.merge(multi_df, on=['gene_lower','position'], how='left')\n",
    "df = df.drop(columns=['gene_lower'])\n",
    "\n",
    "# === Summary columns ===\n",
    "def compute_summary(row):\n",
    "    partners, plddt_v, contact_v, disrupt_v, iface_partners = [], [], [], [], []\n",
    "    for pl in all_partner_labels:\n",
    "        p_col = f\"multi_{pl}_plddt\"\n",
    "        if p_col in row.index and pd.notna(row[p_col]):\n",
    "            partners.append(pl)\n",
    "            plddt_v.append(row[p_col])\n",
    "            c = sf(row.get(f\"multi_{pl}_n_contacts\"), 0)\n",
    "            contact_v.append(c)\n",
    "            d = sf(row.get(f\"multi_{pl}_disruption\"), 0)\n",
    "            disrupt_v.append(d)\n",
    "            if sb(row.get(f\"multi_{pl}_is_interface\"), False):\n",
    "                iface_partners.append(pl)\n",
    "    return pd.Series({\n",
    "        'n_multimer_complexes': len(partners),\n",
    "        'multimer_partners': ';'.join(partners) if partners else None,\n",
    "        'is_interface_any': len(iface_partners) > 0,\n",
    "        'interface_partners': ';'.join(iface_partners) if iface_partners else None,\n",
    "        'n_interface_partners': len(iface_partners),\n",
    "        'multimer_plddt_max': max(plddt_v) if plddt_v else None,\n",
    "        'multimer_plddt_avg': round(np.mean(plddt_v), 2) if plddt_v else None,\n",
    "        'multimer_contacts_max': max(contact_v) if contact_v else None,\n",
    "        'multimer_contacts_avg': round(np.mean(contact_v), 2) if contact_v else None,\n",
    "        'multimer_disruption_max': max(disrupt_v) if disrupt_v else None,\n",
    "        'multimer_disruption_avg': round(np.mean(disrupt_v), 2) if disrupt_v else None,\n",
    "    })\n",
    "\n",
    "summary = df.apply(compute_summary, axis=1)\n",
    "df = pd.concat([df, summary], axis=1)\n",
    "\n",
    "# best_plddt\n",
    "def best_p(row):\n",
    "    vals = []\n",
    "    if pd.notna(row.get('monomer_plddt')): vals.append(row['monomer_plddt'])\n",
    "    for pl in all_partner_labels:\n",
    "        col = f\"multi_{pl}_plddt\"\n",
    "        if col in row.index and pd.notna(row[col]): vals.append(row[col])\n",
    "    return max(vals) if vals else None\n",
    "\n",
    "df['best_plddt'] = df.apply(best_p, axis=1)\n",
    "df['confidence'] = df['best_plddt'].apply(lambda x: 'high' if pd.notna(x) and x >= 70 else ('low' if pd.notna(x) else 'unknown'))\n",
    "\n",
    "print(f\"\\n\u2713 Multimer complete\")\n",
    "print(f\"  Interface variants: {df['is_interface_any'].sum()}/{len(df)}\")\n",
    "print(f\"  With multimer data: {df['n_multimer_complexes'].gt(0).sum()}/{len(df)}\")\n",
    "print(f\"  best_plddt filled:  {df['best_plddt'].notna().sum()}/{len(df)}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n# CELL 6: FOLDX DDG (v5.1)\n# =============================================================================\n# v5.1 changes:\n#   1. Added ddg_multimer_min (most stabilizing complex)\n#   2. Added ddg_confidence flag based on best_pLDDT\n#   3. ddg_category reclassified at low pLDDT (appends _unreliable)\n# =============================================================================\n\nif MONOMER_DDG_FILE.exists():\n    ddg = pd.read_csv(MONOMER_DDG_FILE)\n    ddg.columns = [c.lower() for c in ddg.columns]\n    dc = next((c for c in ['ddg','ddg_monomer','total_ddg'] if c in ddg.columns), None)\n    if dc:\n        ddg = ddg.rename(columns={dc:'ddg_monomer'})\n        df = df.merge(ddg[['gene','position','ref_aa','alt_aa','ddg_monomer']],\n                      on=['gene','position','ref_aa','alt_aa'], how='left')\n        print(f\"\u2713 Monomer DDG: {df['ddg_monomer'].notna().sum()}/{len(df)}\")\nelse:\n    df['ddg_monomer'] = None\n    print(f\"\u26a0 Monomer DDG file not found\")\n\nif MULTIMER_DDG_FILE.exists():\n    ddgm = pd.read_csv(MULTIMER_DDG_FILE)\n    ddgm.columns = [c.lower() for c in ddgm.columns]\n    dc = next((c for c in ['ddg','ddg_multimer','total_ddg'] if c in ddgm.columns), None)\n    if dc:\n        grp = ddgm.groupby(['gene','position','ref_aa','alt_aa'])\n        agg = grp.agg(\n            ddg_multimer_max=(dc,'max'),\n            ddg_multimer_min=(dc,'min'),       # v5.1: most stabilizing complex\n            ddg_multimer_mean=(dc,'mean'),\n            n_complexes_tested=(dc,'count')\n        ).reset_index()\n        if 'partner' in ddgm.columns:\n            pt = grp['partner'].apply(lambda x: ';'.join(x.astype(str))).reset_index()\n            pt.columns = ['gene','position','ref_aa','alt_aa','partners_tested']\n            agg = agg.merge(pt, on=['gene','position','ref_aa','alt_aa'], how='left')\n        df = df.merge(agg, on=['gene','position','ref_aa','alt_aa'], how='left')\n        print(f\"\u2713 Multimer DDG: {df['ddg_multimer_max'].notna().sum()}/{len(df)}\")\n        print(f\"  ddg_multimer_min range: {df['ddg_multimer_min'].min():.2f} to {df['ddg_multimer_min'].max():.2f}\")\nelse:\n    for c in ['ddg_multimer_max','ddg_multimer_min','ddg_multimer_mean','n_complexes_tested','partners_tested']:\n        df[c] = None\n\nfor c in ['ddg_monomer','ddg_multimer_max','ddg_multimer_min','ddg_multimer_mean','n_complexes_tested','partners_tested']:\n    if c not in df.columns: df[c] = None\n\n# -------------------------------------------------------------------------\n# DDG CONFIDENCE (v5.1): based on best_pLDDT\n#   high:     pLDDT >= 70  \u2192 DDG values are reliable\n#   moderate: pLDDT 50-69  \u2192 DDG values are usable with caution\n#   low:      pLDDT < 50   \u2192 DDG values are unreliable (FoldX on bad structure)\n# -------------------------------------------------------------------------\ndef assign_ddg_confidence(plddt):\n    if pd.isna(plddt): return 'unknown'\n    p = float(plddt)\n    if p >= 70: return 'high'\n    elif p >= 50: return 'moderate'\n    else: return 'low'\n\ndf['ddg_confidence'] = df['best_plddt'].apply(assign_ddg_confidence)\n\n# -------------------------------------------------------------------------\n# DDG CATEGORY (v5.1): classify monomer DDG, flag unreliable at low pLDDT\n# -------------------------------------------------------------------------\ndef classify_ddg(v):\n    if pd.isna(v): return None\n    v = float(v)\n    if v > 2.0: return 'highly_destabilizing'\n    elif v > 1.0: return 'destabilizing'\n    elif v > 0.5: return 'mildly_destabilizing'\n    elif v > -0.5: return 'neutral'\n    elif v > -1.0: return 'mildly_stabilizing'\n    elif v > -2.0: return 'stabilizing'\n    else: return 'highly_stabilizing'\n\ndef classify_ddg_with_confidence(row):\n    raw_cat = classify_ddg(row.get('ddg_monomer'))\n    if raw_cat is None: return None\n    confidence = row.get('ddg_confidence', 'unknown')\n    if confidence == 'low':\n        return raw_cat + '_unreliable'\n    return raw_cat\n\ndf['ddg_category_raw'] = df['ddg_monomer'].apply(classify_ddg)\ndf['ddg_category'] = df.apply(classify_ddg_with_confidence, axis=1)\n\nprint(f\"\u2713 DDG complete (v5.1)\")\nprint(f\"\\nDDG confidence distribution:\")\nprint(df['ddg_confidence'].value_counts().to_string())\nprint(f\"\\nDDG category distribution:\")\nprint(df['ddg_category'].value_counts().to_string())\n\n# Show impact of confidence gating\nif df['ddg_category'].notna().any():\n    unreliable = df['ddg_category'].str.contains('_unreliable', na=False).sum()\n    print(f\"\\n  Variants reclassified as unreliable: {unreliable}\")\n    hi_ddg_low_conf = df[(df['ddg_category_raw'].isin(['highly_destabilizing','destabilizing'])) & \n                          (df['ddg_confidence'] == 'low')]\n    if len(hi_ddg_low_conf) > 0:\n        print(f\"  High/destabilizing DDG at low pLDDT (now flagged):\")\n        for _, r in hi_ddg_low_conf.iterrows():\n            print(f\"    {r['gene']} {r['ref_aa']}{int(r['position'])}{r['alt_aa']}: \"\n                  f\"DDG={r['ddg_monomer']:.2f}, pLDDT={r['best_plddt']:.0f}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 6a: FOLDX MONOMER DDG FOR MISSING VARIANTS\n",
    "# =============================================================================\n",
    "\n",
    "import shutil\n",
    "\n",
    "# FoldX binary \u2014 set FOLDX_PATH env var or ensure 'foldx' is on your PATH\n",
    "FOLDX_BINARY = Path(os.environ.get(\"FOLDX_PATH\", shutil.which(\"foldx\") or \"foldx\"))\n",
    "FOLDX_DIR_BASE = WORKING_DIR / \"foldx_expanded\"\n",
    "FOLDX_DIR_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# rotabase.txt should be in the FoldX binary directory\n",
    "ROTABASE = FOLDX_BINARY.parent / \"rotabase.txt\"\n",
    "\n",
    "ONE_TO_THREE = {v:k for k,v in THREE_TO_ONE.items()}\n",
    "\n",
    "def run_foldx_buildmodel(structure_path, chain_id, ref_aa, position, alt_aa, work_dir, n_runs=3):\n",
    "    \"\"\"Run FoldX BuildModel and return DDG. Returns None on failure.\"\"\"\n",
    "    work_dir = Path(work_dir)\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy structure to work dir\n",
    "    struct_name = structure_path.name\n",
    "    shutil.copy2(structure_path, work_dir / struct_name)\n",
    "\n",
    "    # Copy rotabase if needed\n",
    "    if ROTABASE.exists() and not (work_dir / \"rotabase.txt\").exists():\n",
    "        shutil.copy2(ROTABASE, work_dir / \"rotabase.txt\")\n",
    "\n",
    "    # FoldX mutation format: {wt_aa_1letter}{chain}{position}{mut_aa_1letter};\n",
    "    mut_str = f\"{ref_aa}{chain_id}{position}{alt_aa};\"\n",
    "\n",
    "    # Write individual_list.txt\n",
    "    mut_file = work_dir / \"individual_list.txt\"\n",
    "    mut_file.write_text(mut_str + \"\\n\")\n",
    "\n",
    "    # Run FoldX\n",
    "    cmd = [\n",
    "        str(FOLDX_BINARY),\n",
    "        \"--command=BuildModel\",\n",
    "        f\"--pdb={struct_name}\",\n",
    "        \"--mutant-file=individual_list.txt\",\n",
    "        f\"--numberOfRuns={n_runs}\",\n",
    "        f\"--output-dir={work_dir}\",\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300, cwd=str(work_dir))\n",
    "        if result.returncode != 0:\n",
    "            print(f\"    FoldX error: {result.stderr[:200]}\")\n",
    "            return None\n",
    "\n",
    "        # Parse output: Dif_{struct_name}.fxout\n",
    "        dif_file = work_dir / f\"Dif_{struct_name.replace('.pdb','')}.fxout\"\n",
    "        if not dif_file.exists():\n",
    "            # Try alternative naming\n",
    "            for f in work_dir.glob(\"Dif_*.fxout\"):\n",
    "                dif_file = f\n",
    "                break\n",
    "\n",
    "        if not dif_file.exists():\n",
    "            print(f\"    No Dif output found in {work_dir}\")\n",
    "            return None\n",
    "\n",
    "        # Read DDG: skip header lines, take average of runs\n",
    "        ddg_values = []\n",
    "        with open(dif_file) as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line or line.startswith('Pdb') or line.startswith('#'):\n",
    "                    continue\n",
    "                parts = line.split('\\t')\n",
    "                if len(parts) >= 2:\n",
    "                    try:\n",
    "                        ddg_values.append(float(parts[1]))\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "\n",
    "        if ddg_values:\n",
    "            return round(sum(ddg_values) / len(ddg_values), 4)\n",
    "        return None\n",
    "\n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(f\"    FoldX timeout for {ref_aa}{position}{alt_aa}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"    FoldX exception: {e}\")\n",
    "        return None\n",
    "\n",
    "# Identify variants missing monomer DDG\n",
    "missing_mono = df[df['ddg_monomer'].isna()].copy()\n",
    "print(f\"Variants missing monomer DDG: {len(missing_mono)}\")\n",
    "if len(missing_mono) > 0:\n",
    "    print(f\"  Genes: {missing_mono['gene'].value_counts().to_string()}\")\n",
    "\n",
    "# Verify FoldX binary exists\n",
    "if not FOLDX_BINARY.exists():\n",
    "    print(f\"\u26a0 FoldX binary not found at {FOLDX_BINARY}\")\n",
    "    print(\"  Skipping FoldX computation. Set FOLDX_BINARY path and re-run this cell.\")\n",
    "else:\n",
    "    new_ddg_mono = {}\n",
    "    for idx, row in missing_mono.iterrows():\n",
    "        gene = str(row['gene']).lower()\n",
    "        pos = int(row['position'])\n",
    "        ref = str(row['ref_aa'])\n",
    "        alt = str(row['alt_aa'])\n",
    "\n",
    "        # Find PDB structure\n",
    "        _, pdb_name = MONOMER_STRUCTURES.get(gene, (None, None))\n",
    "        pdb_path = find_file(pdb_name)\n",
    "        if pdb_path is None:\n",
    "            print(f\"  \u26a0 No monomer PDB for {gene} \u2014 skipping {ref}{pos}{alt}\")\n",
    "            continue\n",
    "\n",
    "        work_dir = FOLDX_DIR_BASE / \"monomer\" / f\"{gene}_{ref}{pos}{alt}\"\n",
    "        print(f\"  Running FoldX: {gene} {ref}{pos}{alt}...\", end=\" \")\n",
    "        ddg = run_foldx_buildmodel(pdb_path, 'A', ref, pos, alt, work_dir)\n",
    "        if ddg is not None:\n",
    "            new_ddg_mono[idx] = ddg\n",
    "            print(f\"DDG = {ddg:.2f}\")\n",
    "        else:\n",
    "            print(\"FAILED\")\n",
    "\n",
    "    # Merge new DDG values\n",
    "    for idx, ddg in new_ddg_mono.items():\n",
    "        df.loc[idx, 'ddg_monomer'] = ddg\n",
    "        df.loc[idx, 'ddg_category'] = classify_ddg(ddg)\n",
    "\n",
    "    print(f\"\\n\u2713 Computed {len(new_ddg_mono)} new monomer DDG values\")\n",
    "    print(f\"  Total monomer DDG coverage: {df['ddg_monomer'].notna().sum()}/{len(df)}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 6b: FOLDX MULTIMER DDG FOR MISSING VARIANTS\n",
    "# =============================================================================\n",
    "\n",
    "# For multimer DDG, run BuildModel on each complex PDB where the variant\n",
    "# is at the interface, then compute interaction energy difference via AnalyseComplex.\n",
    "\n",
    "def run_foldx_analysecomplex(structure_path, chains, work_dir):\n",
    "    \"\"\"Run FoldX AnalyseComplex and return interaction energy.\"\"\"\n",
    "    work_dir = Path(work_dir)\n",
    "    struct_name = structure_path.name\n",
    "    if not (work_dir / struct_name).exists():\n",
    "        shutil.copy2(structure_path, work_dir / struct_name)\n",
    "    if ROTABASE.exists() and not (work_dir / \"rotabase.txt\").exists():\n",
    "        shutil.copy2(ROTABASE, work_dir / \"rotabase.txt\")\n",
    "\n",
    "    cmd = [\n",
    "        str(FOLDX_BINARY),\n",
    "        \"--command=AnalyseComplex\",\n",
    "        f\"--pdb={struct_name}\",\n",
    "        f\"--analyseComplexChains={chains}\",\n",
    "        f\"--output-dir={work_dir}\",\n",
    "    ]\n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=300, cwd=str(work_dir))\n",
    "        if result.returncode != 0:\n",
    "            return None\n",
    "\n",
    "        # Parse Interaction_{name}_AC.fxout\n",
    "        for f in work_dir.glob(\"Interaction_*_AC.fxout\"):\n",
    "            with open(f) as fh:\n",
    "                for line in fh:\n",
    "                    if line.startswith('Pdb') or line.startswith('#') or not line.strip():\n",
    "                        continue\n",
    "                    parts = line.strip().split('\\t')\n",
    "                    if len(parts) >= 6:\n",
    "                        try:\n",
    "                            return float(parts[5])  # Interaction Energy\n",
    "                        except ValueError:\n",
    "                            continue\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "\n",
    "def run_foldx_multimer_ddg(pdb_path, chain_gene, chain_partner, ref_aa, position, alt_aa, work_dir, n_runs=3):\n",
    "    \"\"\"Compute multimer DDG: BuildModel then AnalyseComplex on WT and mutant.\"\"\"\n",
    "    work_dir = Path(work_dir)\n",
    "    work_dir.mkdir(parents=True, exist_ok=True)\n",
    "    chains_str = f\"{chain_gene},{chain_partner}\"\n",
    "\n",
    "    # Step 1: AnalyseComplex on wild-type\n",
    "    wt_dir = work_dir / \"wt\"\n",
    "    wt_dir.mkdir(exist_ok=True)\n",
    "    ie_wt = run_foldx_analysecomplex(pdb_path, chains_str, wt_dir)\n",
    "\n",
    "    # Step 2: BuildModel to get mutant structure\n",
    "    ddg_fold = run_foldx_buildmodel(pdb_path, chain_gene, ref_aa, position, alt_aa, work_dir, n_runs)\n",
    "\n",
    "    # Step 3: Find mutant PDB and run AnalyseComplex\n",
    "    struct_base = pdb_path.stem\n",
    "    mutant_pdb = None\n",
    "    for f in work_dir.glob(f\"{struct_base}_1_*.pdb\"):\n",
    "        mutant_pdb = f; break\n",
    "    if mutant_pdb is None:\n",
    "        for f in work_dir.glob(\"*_1.pdb\"):\n",
    "            mutant_pdb = f; break\n",
    "\n",
    "    ie_mut = None\n",
    "    if mutant_pdb is not None:\n",
    "        mut_dir = work_dir / \"mut\"\n",
    "        mut_dir.mkdir(exist_ok=True)\n",
    "        ie_mut = run_foldx_analysecomplex(mutant_pdb, chains_str, mut_dir)\n",
    "\n",
    "    # DDG_binding = IE(mutant) - IE(wt)\n",
    "    ddg_binding = None\n",
    "    if ie_wt is not None and ie_mut is not None:\n",
    "        ddg_binding = round(ie_mut - ie_wt, 4)\n",
    "\n",
    "    return ddg_fold, ddg_binding\n",
    "\n",
    "\n",
    "# Identify interface variants missing multimer DDG\n",
    "if not FOLDX_BINARY.exists():\n",
    "    print(\"\u26a0 FoldX binary not found \u2014 skipping multimer DDG\")\n",
    "else:\n",
    "    # Variants at interfaces that don't have multimer DDG\n",
    "    missing_multi = df[df['is_interface_any'] & df['ddg_multimer_max'].isna()].copy()\n",
    "    print(f\"Interface variants missing multimer DDG: {len(missing_multi)}\")\n",
    "\n",
    "    new_multi_results = []\n",
    "\n",
    "    for idx, row in missing_multi.iterrows():\n",
    "        gene = str(row['gene']).lower()\n",
    "        pos = int(row['position'])\n",
    "        ref = str(row['ref_aa'])\n",
    "        alt = str(row['alt_aa'])\n",
    "        partners = str(row.get('interface_partners', ''))\n",
    "        if not partners or partners == 'nan':\n",
    "            continue\n",
    "\n",
    "        for partner in partners.split(';'):\n",
    "            partner = partner.strip()\n",
    "            if not partner:\n",
    "                continue\n",
    "\n",
    "            # Find the matching multimer definition\n",
    "            matched = None\n",
    "            for g1, pl, cif, pdb, c1, c2, primary in MULTIMER_STRUCTURES:\n",
    "                if g1 == gene and pl == partner:\n",
    "                    matched = (pdb, c1, c2)\n",
    "                    break\n",
    "                # Check reverse: partner gene with gene as partner\n",
    "                partner_gene_map = {\n",
    "                    'dvl2':'dvl2','ctnnb1':'ctnnb1','rock2':'rock2',\n",
    "                    'gli3':'gli3','kpna1':'kpna1','kpna6':'kpna6',\n",
    "                    'mdfi':'mdfi','tcf7l1':'tcf7l1',\n",
    "                    'cdh2_truncated':'cdh2','cdh2_cyto':'cdh2',\n",
    "                    'actin':'actb','actb':'actb','actb_no_bind':'actb',\n",
    "                }\n",
    "                pg = partner_gene_map.get(pl, pl)\n",
    "                if g1 != gene and pg == gene and pl == partner:\n",
    "                    matched = (pdb, c2, c1)  # Swap chains\n",
    "                    break\n",
    "\n",
    "            if matched is None:\n",
    "                continue\n",
    "\n",
    "            pdb_file, my_chain, partner_chain = matched\n",
    "            pdb_path = find_file(pdb_file)\n",
    "            if pdb_path is None:\n",
    "                continue\n",
    "\n",
    "            work_dir = FOLDX_DIR_BASE / \"multimer\" / f\"{gene}_{ref}{pos}{alt}_{partner}\"\n",
    "            print(f\"  Running FoldX multimer: {gene} {ref}{pos}{alt} \u00d7 {partner}...\", end=\" \")\n",
    "\n",
    "            ddg_fold, ddg_binding = run_foldx_multimer_ddg(\n",
    "                pdb_path, my_chain, partner_chain, ref, pos, alt, work_dir\n",
    "            )\n",
    "\n",
    "            ddg_val = ddg_binding if ddg_binding is not None else ddg_fold\n",
    "            if ddg_val is not None:\n",
    "                new_multi_results.append({\n",
    "                    'idx': idx, 'gene': gene, 'position': pos,\n",
    "                    'ref_aa': ref, 'alt_aa': alt, 'partner': partner,\n",
    "                    'ddg_multimer': ddg_val, 'ddg_fold': ddg_fold, 'ddg_binding': ddg_binding\n",
    "                })\n",
    "                print(f\"DDG_binding={ddg_binding}, DDG_fold={ddg_fold}\")\n",
    "            else:\n",
    "                print(\"FAILED\")\n",
    "\n",
    "    # Aggregate and merge\n",
    "    if new_multi_results:\n",
    "        multi_new = pd.DataFrame(new_multi_results)\n",
    "        grp = multi_new.groupby('idx').agg(\n",
    "            ddg_max=('ddg_multimer', 'max'),\n",
    "            ddg_mean=('ddg_multimer', 'mean'),\n",
    "            n_tested=('ddg_multimer', 'count'),\n",
    "            partners=('partner', lambda x: ';'.join(x))\n",
    "        )\n",
    "        for idx, row in grp.iterrows():\n",
    "            existing_max = df.loc[idx, 'ddg_multimer_max']\n",
    "            if pd.isna(existing_max) or row['ddg_max'] > existing_max:\n",
    "                df.loc[idx, 'ddg_multimer_max'] = row['ddg_max']\n",
    "            existing_mean = df.loc[idx, 'ddg_multimer_mean']\n",
    "            if pd.isna(existing_mean):\n",
    "                df.loc[idx, 'ddg_multimer_mean'] = row['ddg_mean']\n",
    "            existing_n = df.loc[idx, 'n_complexes_tested']\n",
    "            if pd.isna(existing_n):\n",
    "                df.loc[idx, 'n_complexes_tested'] = row['n_tested']\n",
    "            else:\n",
    "                df.loc[idx, 'n_complexes_tested'] = int(existing_n) + row['n_tested']\n",
    "            existing_p = str(df.loc[idx, 'partners_tested'])\n",
    "            if existing_p == 'nan' or not existing_p:\n",
    "                df.loc[idx, 'partners_tested'] = row['partners']\n",
    "            else:\n",
    "                df.loc[idx, 'partners_tested'] = existing_p + ';' + row['partners']\n",
    "\n",
    "        print(f\"\\n\u2713 Computed {len(new_multi_results)} new multimer DDG values across {len(grp)} variants\")\n",
    "    else:\n",
    "        print(\"  No new multimer DDG computed\")\n",
    "\n",
    "    # Save expanded DDG results for future re-use\n",
    "    foldx_out = RESULTS_DIR / \"foldx_ddg_expanded_results.csv\"\n",
    "    ddg_cols_save = ['gene','position','ref_aa','alt_aa','ddg_monomer','ddg_category',\n",
    "                     'ddg_multimer_max','ddg_multimer_mean','n_complexes_tested','partners_tested']\n",
    "    df[[c for c in ddg_cols_save if c in df.columns]].to_csv(foldx_out, index=False)\n",
    "    print(f\"  Saved expanded DDG to {foldx_out}\")\n",
    "\n",
    "print(f\"\\nFinal DDG coverage:\")\n",
    "print(f\"  Monomer DDG: {df['ddg_monomer'].notna().sum()}/{len(df)}\")\n",
    "print(f\"  Multimer DDG: {df['ddg_multimer_max'].notna().sum()}/{len(df)}\")\n",
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 7: SCORING, TIERS, MECHANISM (v5.1)\n",
    "# =============================================================================\n",
    "# v5 changes:\n",
    "#   1. Disruption = substitution_severity \u00d7 (monomer_contacts + \u03a3 inter_contacts)\n",
    "#   2. Disruption thresholds: \u226520\u2192+4, \u226510\u2192+3, \u22654\u2192+2, \u22651\u2192+1\n",
    "#   3. Burial: best across monomer + confident multimer (pLDDT \u2265 50)\n",
    "#   4. Grantham bonus removed (integrated into disruption)\n",
    "#   5. pLDDT multiplier graduated: \u226570\u2192\u00d71.0, 50-69\u2192\u00d70.7, <50\u2192\u00d70.4\n",
    "# v5.1 changes:\n",
    "#   6. Mechanism: DDG-based classifications gated by pLDDT (ddg_confidence)\n",
    "#   7. Low-pLDDT variants with high DDG \u2192 'DDG unreliable (low confidence)'\n",
    "#   8. Stabilizing DDG recognized in mechanism (highly_stabilizing category)\n",
    "# =============================================================================\n",
    "\n",
    "BURIAL_RANK = {'unknown': 0, 'surface_exposed': 1, 'partially_buried': 2, 'buried_core': 3}\n",
    "RANK_TO_BURIAL = {v: k for k, v in BURIAL_RANK.items()}\n",
    "\n",
    "\n",
    "def calculate_score(row):\n",
    "    \"\"\"\n",
    "    Structural disruption score (v5).\n",
    "    \n",
    "    final_score = (disruption_pts + interface_pts + burial_pts) \u00d7 pLDDT_multiplier\n",
    "    \n",
    "    Max possible: (4 + 2 + 2) \u00d7 1.0 = 8.0\n",
    "    \"\"\"\n",
    "    score, ev = 0.0, []\n",
    "\n",
    "    # =================================================================\n",
    "    # DISRUPTION: substitution_severity \u00d7 (mono_contacts + \u03a3 inter_contacts)\n",
    "    # =================================================================\n",
    "    sev = sf(row.get('substitution_severity'), 0)\n",
    "    mono_c = sf(row.get('monomer_n_contacts'), 0)\n",
    "\n",
    "    # Sum inter-chain contacts across ALL multimer complexes\n",
    "    inter_sum = 0.0\n",
    "    inter_details = []\n",
    "    for pl in all_partner_labels:\n",
    "        ic_col = f\"multi_{pl}_inter_contacts\"\n",
    "        if ic_col in row.index and pd.notna(row[ic_col]):\n",
    "            ic_val = float(row[ic_col])\n",
    "            if ic_val > 0:\n",
    "                inter_sum += ic_val\n",
    "                inter_details.append(f\"{pl}:{int(ic_val)}\")\n",
    "\n",
    "    total_contacts = mono_c + inter_sum\n",
    "    disruption = round(sev * total_contacts, 2)\n",
    "\n",
    "    # Disruption \u2192 points (thresholds calibrated on high-confidence distribution)\n",
    "    if   disruption >= 20: score += 4.0; ev.append(f'very_high_disruption({disruption:.1f})')\n",
    "    elif disruption >= 10: score += 3.0; ev.append(f'high_disruption({disruption:.1f})')\n",
    "    elif disruption >= 4:  score += 2.0; ev.append(f'moderate_disruption({disruption:.1f})')\n",
    "    elif disruption >= 1:  score += 1.0; ev.append(f'low_disruption({disruption:.1f})')\n",
    "    else:\n",
    "        ev.append(f'no_disruption({disruption:.1f})')\n",
    "\n",
    "    # =================================================================\n",
    "    # INTERFACE POINTS (unchanged from v4)\n",
    "    # =================================================================\n",
    "    ni = si(row.get('n_interface_partners'), 0)\n",
    "    ip = ss(row.get('interface_partners'))\n",
    "    if   ni >= 2: score += 2.0; ev.append(f'multi_interface({ni})')\n",
    "    elif ni == 1: score += 1.5; ev.append(f'interface({ip})')\n",
    "\n",
    "    # =================================================================\n",
    "    # BURIAL: best across monomer + confident multimer (pLDDT \u2265 50)\n",
    "    # =================================================================\n",
    "    mono_burial = ss(row.get('monomer_burial'))\n",
    "    best_rank = BURIAL_RANK.get(mono_burial, 0)\n",
    "    best_source = 'monomer'\n",
    "\n",
    "    for pl in all_partner_labels:\n",
    "        burial_col = f\"multi_{pl}_burial\"\n",
    "        plddt_col  = f\"multi_{pl}_plddt\"\n",
    "        if burial_col in row.index and pd.notna(row.get(burial_col)):\n",
    "            pl_plddt = sf(row.get(plddt_col), 0)\n",
    "            if pl_plddt >= 50:  # only trust multimer burial if confident\n",
    "                pl_burial = ss(row[burial_col])\n",
    "                pl_rank = BURIAL_RANK.get(pl_burial, 0)\n",
    "                if pl_rank > best_rank:\n",
    "                    best_rank = pl_rank\n",
    "                    best_source = pl\n",
    "\n",
    "    best_burial = RANK_TO_BURIAL.get(best_rank, 'unknown')\n",
    "    if   best_burial == 'buried_core':      score += 2.0; ev.append(f'buried_core({best_source})')\n",
    "    elif best_burial == 'partially_buried': score += 1.0; ev.append(f'partially_buried({best_source})')\n",
    "\n",
    "    # =================================================================\n",
    "    # pLDDT MULTIPLIER (graduated)\n",
    "    # =================================================================\n",
    "    bp = row.get('best_plddt')\n",
    "    if bp is not None and not pd.isna(bp):\n",
    "        bp_val = float(bp)\n",
    "        if bp_val < 50:\n",
    "            score *= 0.4\n",
    "            ev.append(f'very_low_plddt_discount({int(bp_val)})')\n",
    "        elif bp_val < 70:\n",
    "            score *= 0.7\n",
    "            ev.append(f'low_plddt_discount({int(bp_val)})')\n",
    "        # else: \u00d71.0, no discount\n",
    "\n",
    "    final = round(score, 2)\n",
    "    return (final, ';'.join(ev), disruption, total_contacts, inter_sum, best_burial, best_source)\n",
    "\n",
    "\n",
    "def assign_tier(s):\n",
    "    if pd.isna(s): return 'Tier 4 - Likely benign'\n",
    "    s = float(s)\n",
    "    if   s >= 5.0: return 'Tier 1 - High confidence pathogenic'\n",
    "    elif s >= 3.0: return 'Tier 2 - Likely pathogenic'\n",
    "    elif s >= 1.5: return 'Tier 3 - VUS with evidence'\n",
    "    else:          return 'Tier 4 - Likely benign'\n",
    "\n",
    "\n",
    "def classify_mechanism(row):\n",
    "    \"\"\"\n",
    "    FoldX DDG mechanism classification (v5.1).\n",
    "    \n",
    "    Changes from v5:\n",
    "      - DDG-based classifications gated by ddg_confidence\n",
    "      - Low-pLDDT variants \u2192 'DDG unreliable (low confidence)' instead of mechanism credit\n",
    "      - Stabilizing DDG (< -2) recognized as potential gain-of-function mechanism\n",
    "    \"\"\"\n",
    "    tier     = ss(row.get('tier'))\n",
    "    ddg_m    = row.get('ddg_monomer')\n",
    "    ddg_x    = row.get('ddg_multimer_max')\n",
    "    ddg_min  = row.get('ddg_multimer_min')\n",
    "    is_if    = sb(row.get('is_interface_any'), False)\n",
    "    ht       = 'Tier 1' in tier or 'Tier 2' in tier\n",
    "    ddg_conf = ss(row.get('ddg_confidence'))\n",
    "\n",
    "    has_dm = pd.notna(ddg_m)\n",
    "    has_dx = pd.notna(ddg_x)\n",
    "\n",
    "    # ---- pLDDT gate: if low confidence, DDG is unreliable ----\n",
    "    if ddg_conf == 'low' and (has_dm or has_dx):\n",
    "        any_extreme = (has_dm and abs(float(ddg_m)) > 2.0) or (has_dx and abs(float(ddg_x)) > 2.0)\n",
    "        if any_extreme:\n",
    "            return 'DDG unreliable (low confidence)'\n",
    "        # Mild DDG at low pLDDT \u2014 still unreliable but not worth flagging\n",
    "        if ht:\n",
    "            if is_if:\n",
    "                return 'Interface disruption (DDG-neutral)'\n",
    "            else:\n",
    "                return 'Structural tier (DDG unreliable)'\n",
    "        else:\n",
    "            if not has_dm and not has_dx:\n",
    "                return 'No DDG data'\n",
    "            return 'Likely benign'\n",
    "\n",
    "    # ---- Reliable DDG (pLDDT >= 50) ----\n",
    "    # Monomer DDG thresholds\n",
    "    hi_dm   = has_dm and float(ddg_m) > 2.0\n",
    "    neut_dm = has_dm and abs(float(ddg_m)) < 1.0\n",
    "    mod_dm  = has_dm and 0.5 <= float(ddg_m) <= 1.5\n",
    "    stab_dm = has_dm and float(ddg_m) < -2.0  # v5.1: highly stabilizing\n",
    "\n",
    "    # Multimer DDG thresholds\n",
    "    hi_dx   = has_dx and float(ddg_x) > 2.0\n",
    "    mod_dx  = has_dx and 0.5 <= float(ddg_x) <= 1.5\n",
    "    stab_dx = has_dx and pd.notna(ddg_min) and float(ddg_min) < -2.0  # v5.1: use min for stabilizing\n",
    "\n",
    "    if ht:\n",
    "        # High-tier mechanisms (Tier 1 or 2)\n",
    "        if hi_dm and is_if and hi_dx:\n",
    "            return 'Dual mechanism (fold + PPI)'\n",
    "        elif hi_dm and is_if:\n",
    "            return 'Dual mechanism (fold + interface)'\n",
    "        elif neut_dm and is_if and hi_dx:\n",
    "            return 'Complex destabilization (PPI-specific)'\n",
    "        elif is_if and not hi_dm:\n",
    "            return 'Interface disruption (DDG-neutral)'\n",
    "        elif hi_dm:\n",
    "            return 'Fold destabilization'\n",
    "        elif stab_dm or stab_dx:\n",
    "            return 'Potential gain-of-function (over-stabilization)'\n",
    "        elif mod_dm or mod_dx:\n",
    "            return 'Structural concern (moderate DDG)'\n",
    "        elif has_dm or has_dx:\n",
    "            return 'Structural tier (low DDG)'\n",
    "        else:\n",
    "            return 'No DDG data'\n",
    "    else:\n",
    "        # Low-tier mechanisms (Tier 3 or 4)\n",
    "        if hi_dm or hi_dx:\n",
    "            return 'High DDG only (Tier 3/4)'\n",
    "        elif stab_dm or stab_dx:\n",
    "            return 'High stabilizing DDG only (Tier 3/4)'\n",
    "        elif not has_dm and not has_dx:\n",
    "            return 'No DDG data'\n",
    "        else:\n",
    "            return 'Likely benign'\n",
    "\n",
    "\n",
    "# === Apply scoring ===\n",
    "results = df.apply(lambda r: calculate_score(r), axis=1)\n",
    "df['final_score']       = [r[0] for r in results]\n",
    "df['score_evidence']    = [r[1] for r in results]\n",
    "df['contact_disruption']= [r[2] for r in results]\n",
    "df['total_contacts']    = [r[3] for r in results]\n",
    "df['inter_contacts_sum']= [r[4] for r in results]\n",
    "df['best_burial']       = [r[5] for r in results]\n",
    "df['best_burial_source']= [r[6] for r in results]\n",
    "\n",
    "df['tier'] = df['final_score'].apply(assign_tier)\n",
    "df['final_mechanism'] = df.apply(classify_mechanism, axis=1)\n",
    "df['pathogenic_mechanism'] = df['final_mechanism']\n",
    "\n",
    "# === Summary statistics ===\n",
    "print(\"\u2713 Scoring complete (v5)\")\n",
    "print(f\"\\nTier distribution:\")\n",
    "print(df['tier'].value_counts().to_string())\n",
    "print(f\"\\nScore statistics:\")\n",
    "print(f\"  Range: {df['final_score'].min():.2f} \u2013 {df['final_score'].max():.2f}\")\n",
    "print(f\"  Mean:  {df['final_score'].mean():.2f}\")\n",
    "print(f\"\\nDisruption statistics:\")\n",
    "print(f\"  Range: {df['contact_disruption'].min():.1f} \u2013 {df['contact_disruption'].max():.1f}\")\n",
    "print(f\"  Non-zero: {(df['contact_disruption'] > 0).sum()}/{len(df)}\")\n",
    "print(f\"\\nBurial upgrades (multimer > monomer):\")\n",
    "upgraded = df[df['best_burial_source'] != 'monomer']\n",
    "print(f\"  {len(upgraded)}/{len(df)} variants use multimer burial\")\n",
    "print(f\"\\npLDDT multiplier distribution:\")\n",
    "for label, lo, hi in [('\u00d71.0 (\u226570)', 70, 999), ('\u00d70.7 (50-69)', 50, 70), ('\u00d70.4 (<50)', 0, 50)]:\n",
    "    n = ((df['best_plddt'] >= lo) & (df['best_plddt'] < hi)).sum()\n",
    "    print(f\"  {label}: {n}\")\n",
    "print(f\"\\nMechanism classification:\")\n",
    "print(df['final_mechanism'].value_counts().to_string())\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 8: ANNOTATIONS + CONCORDANCE (v5.2)\n",
    "# =============================================================================\n",
    "# v5.2 changes:\n",
    "#   1. DDG vote uses max(|mono|, |multi_max|, |multi_min|) \u2014 fixes multi_max bug\n",
    "#   2. DDG confidence gating via ddg_confidence column (not raw pLDDT)\n",
    "#   3. New sub-score columns: structure_strict/relaxed, external_strict/relaxed\n",
    "#   4. T3-inclusive variants of all sub-scores\n",
    "# =============================================================================\n",
    "\n",
    "df['gene_lower'] = df['gene'].astype(str).str.lower()\n",
    "if annotation_df is not None:\n",
    "    ann = annotation_df.copy()\n",
    "    ann.columns = [c.lower() for c in ann.columns]\n",
    "    ann['gene_lower'] = ann['gene'].astype(str).str.lower()\n",
    "    for col in ['alphamissense','alphamissense_pathogenicity','franklin']:\n",
    "        if col in ann.columns:\n",
    "            m = ann[['gene_lower','position','ref_aa','alt_aa',col]].drop_duplicates()\n",
    "            df = df.merge(m, on=['gene_lower','position','ref_aa','alt_aa'], how='left')\n",
    "            rn = {'alphamissense':'AlphaMissense','alphamissense_pathogenicity':'AlphaMissense_pathogenicity'}\n",
    "            if col in rn: df = df.rename(columns={col: rn[col]})\n",
    "            print(f\"\u2713 {col}: {df[rn.get(col,col)].notna().sum()}/{len(df)}\")\n",
    "\n",
    "for col in ['AlphaMissense','AlphaMissense_pathogenicity','franklin']:\n",
    "    if col not in df.columns: df[col] = None\n",
    "\n",
    "df = df.drop(columns=['gene_lower'], errors='ignore')\n",
    "\n",
    "def build_evidence(row):\n",
    "    parts = []\n",
    "    ddg_m = row.get('ddg_monomer')\n",
    "    if pd.notna(ddg_m):\n",
    "        v = float(ddg_m)\n",
    "        if v > 2.0: parts.append(f'DDG_high({v:.1f})')\n",
    "        elif v > 1.0: parts.append(f'DDG_mod({v:.1f})')\n",
    "    ip = ss(row.get('interface_partners'))\n",
    "    if ip: parts.append(f'interface({ip})')\n",
    "    if ss(row.get('monomer_burial')) == 'buried_core': parts.append('buried')\n",
    "    am = ss(row.get('AlphaMissense')).lower()\n",
    "    if am == 'likely_pathogenic': parts.append('AM_path')\n",
    "    elif am == 'ambiguous': parts.append('AM_amb')\n",
    "    return '; '.join(parts) if parts else 'Limited evidence'\n",
    "\n",
    "df['evidence_summary'] = df.apply(build_evidence, axis=1)\n",
    "\n",
    "def classify_integrated(row):\n",
    "    tier, am, conf = ss(row.get('tier')), ss(row.get('AlphaMissense')).lower(), ss(row.get('confidence'))\n",
    "    is_if = sb(row.get('is_interface_any'), False)\n",
    "    ht = 'Tier 1' in tier or 'Tier 2' in tier\n",
    "    am_p, am_a = am == 'likely_pathogenic', am == 'ambiguous'\n",
    "    if ht and am_p: return 'Class A - Concordant pathogenic'\n",
    "    elif ht and am_a and is_if: return 'Class B - Likely pathogenic (structural)'\n",
    "    elif ht and is_if and conf == 'high': return 'Class C - Interface disruptor (structural only)'\n",
    "    elif ht and conf == 'high': return 'Class D - Structural evidence only (high confidence)'\n",
    "    elif ht and conf == 'low': return 'Class E - Structural evidence only (low confidence)'\n",
    "    elif am_p and 'Tier 3' in tier: return 'Class F - AlphaMissense pathogenic (weak structural)'\n",
    "    elif am_a and ht: return 'Class G - VUS (mixed evidence)'\n",
    "    elif ht: return 'Class H - VUS (weak evidence)'\n",
    "    elif am_p: return 'Class I - AlphaMissense only'\n",
    "    else: return 'Class J - Likely benign'\n",
    "\n",
    "df['integrated_class'] = df.apply(classify_integrated, axis=1)\n",
    "\n",
    "def std_franklin(v):\n",
    "    if pd.isna(v): return 'No data'\n",
    "    v = str(v).strip(); vl = v.lower()\n",
    "    if 'pathogenic' in vl and 'likely' in vl: return 'Likely pathogenic'\n",
    "    elif 'pathogenic' in vl: return 'Pathogenic'\n",
    "    elif 'benign' in vl and 'likely' in vl: return 'Likely benign'\n",
    "    elif 'benign' in vl: return 'Benign'\n",
    "    elif 'vus' in vl and 'high' in vl: return 'VUS (high)'\n",
    "    elif 'vus' in vl and 'mid' in vl: return 'VUS (mid)'\n",
    "    elif 'vus' in vl and 'low' in vl: return 'VUS (low)'\n",
    "    elif 'vus' in vl: return 'VUS'\n",
    "    return v\n",
    "\n",
    "# --- Normalize AlphaMissense: classify raw scores ---\n",
    "def classify_am(val):\n",
    "    if pd.isna(val): return val\n",
    "    s = str(val).strip()\n",
    "    if s in ('likely_pathogenic','likely_benign','ambiguous'): return s\n",
    "    try:\n",
    "        score = float(s)\n",
    "        if score >= 0.564: return 'likely_pathogenic'\n",
    "        elif score < 0.340: return 'likely_benign'\n",
    "        else: return 'ambiguous'\n",
    "    except ValueError:\n",
    "        return s\n",
    "\n",
    "if 'AlphaMissense' in df.columns:\n",
    "    df['AlphaMissense_raw'] = df['AlphaMissense']\n",
    "    df['AlphaMissense'] = df['AlphaMissense'].apply(classify_am)\n",
    "    print(\"AlphaMissense normalized:\", df['AlphaMissense'].value_counts().to_string())\n",
    "\n",
    "# --- Normalize Franklin: fix casing and typos ---\n",
    "if 'franklin' in df.columns:\n",
    "    df['franklin_raw'] = df['franklin']\n",
    "    def norm_franklin(v):\n",
    "        if pd.isna(v): return v\n",
    "        s = str(v).strip().lower()\n",
    "        m = {'benign':'benign','likely benign':'likely benign',\n",
    "             'vus (low)':'VUS (low)','vus (mid)':'VUS (mid)',\n",
    "             'vus mid)':'VUS (mid)','vus(mid)':'VUS (mid)',\n",
    "             'vus (high)':'VUS (high)',\n",
    "             'pathogenic':'pathogenic','likely pathogenic':'likely pathogenic'}\n",
    "        return m.get(s, v)\n",
    "    df['franklin'] = df['franklin'].apply(norm_franklin)\n",
    "    print(\"Franklin normalized:\", df['franklin'].value_counts().to_string())\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CONCORDANCE (v5.2)\n",
    "# =============================================================================\n",
    "# v5.2 fixes:\n",
    "#   - DDG vote: max(|mono|, |multi_max|, |multi_min|) with confidence gating\n",
    "#     (v5.1 bug: only checked multi_max positive, multi_min negative)\n",
    "#   - New sub-score columns for transparent decomposition\n",
    "#\n",
    "# Four evidence lines:\n",
    "#   1. Structural tier:  1 if Tier 1/2 (or Tier 3 for T3-inclusive)\n",
    "#   2. DDG:              1 if max_abs_ddg >= threshold AND confidence passes\n",
    "#   3. AlphaMissense:    1 if pathogenic (strict) or ambiguous+ (relaxed)\n",
    "#   4. Franklin:         1 if VUS(high)+ (strict) or VUS(mid)+ (relaxed)\n",
    "#\n",
    "# DDG thresholds:\n",
    "#   Standard: max_abs_ddg >= 2.0, ddg_confidence = 'high'\n",
    "#   Relaxed:  max_abs_ddg >= 1.0, ddg_confidence != 'low'\n",
    "# =============================================================================\n",
    "\n",
    "def concordance_v52(row):\n",
    "    tier = ss(row.get('tier'))\n",
    "    am = ss(row.get('AlphaMissense')).lower()\n",
    "    fr = std_franklin(row.get('franklin'))\n",
    "    fr_lower = fr.lower() if isinstance(fr, str) else ''\n",
    "    ddg_conf = ss(row.get('ddg_confidence')).lower()\n",
    "\n",
    "    # --- Max absolute DDG across all sources (v5.2 FIX) ---\n",
    "    ddg_vals = []\n",
    "    for col in ['ddg_monomer', 'ddg_multimer_max', 'ddg_multimer_min']:\n",
    "        v = row.get(col)\n",
    "        if pd.notna(v):\n",
    "            ddg_vals.append(abs(float(v)))\n",
    "    max_abs_ddg = max(ddg_vals) if ddg_vals else 0.0\n",
    "\n",
    "    # --- Tier votes ---\n",
    "    tier_t12 = 'Tier 1' in tier or 'Tier 2' in tier\n",
    "    tier_t123 = tier_t12 or 'Tier 3' in tier\n",
    "\n",
    "    # --- DDG votes (v5.2: uses max_abs_ddg + ddg_confidence column) ---\n",
    "    ddg_vote_strict  = 1 if ddg_conf == 'high' and max_abs_ddg >= 2.0 else 0\n",
    "    ddg_vote_relaxed = 1 if ddg_conf != 'low'  and max_abs_ddg >= 1.0 else 0\n",
    "\n",
    "    # --- AlphaMissense votes ---\n",
    "    am_strict  = 1 if am == 'likely_pathogenic' else 0\n",
    "    am_relaxed = 1 if am in ('likely_pathogenic', 'ambiguous') else 0\n",
    "\n",
    "    # --- Franklin votes ---\n",
    "    fr_strict  = 1 if fr_lower in ('vus (high)', 'pathogenic', 'likely pathogenic') else 0\n",
    "    fr_relaxed = 1 if fr_lower in ('vus (high)', 'vus (mid)', 'pathogenic', 'likely pathogenic') else 0\n",
    "\n",
    "    # --- Sub-scores (new in v5.2) ---\n",
    "    tv = 1 if tier_t12 else 0\n",
    "    tv3 = 1 if tier_t123 else 0\n",
    "\n",
    "    struct_s = tv + ddg_vote_strict\n",
    "    struct_r = tv + ddg_vote_relaxed\n",
    "    struct_s_t3 = tv3 + ddg_vote_strict\n",
    "    struct_r_t3 = tv3 + ddg_vote_relaxed\n",
    "    ext_s = am_strict + fr_strict\n",
    "    ext_r = am_relaxed + fr_relaxed\n",
    "\n",
    "    # --- Three-way labels (for backward compat) ---\n",
    "    n3 = tv + am_strict + fr_strict\n",
    "    labels3 = ['All 3 benign/VUS', '1 of 3 pathogenic',\n",
    "               '2 of 3 pathogenic', 'All 3 agree pathogenic']\n",
    "    n3_t3 = tv3 + am_strict + fr_strict\n",
    "\n",
    "    return pd.Series({\n",
    "        # Sub-scores (new in v5.2)\n",
    "        'structure_strict':      struct_s,\n",
    "        'structure_relaxed':     struct_r,\n",
    "        'structure_strict_t3':   struct_s_t3,\n",
    "        'structure_relaxed_t3':  struct_r_t3,\n",
    "        'external_strict':       ext_s,\n",
    "        'external_relaxed':      ext_r,\n",
    "\n",
    "        # Three-way concordance\n",
    "        'three_way':             labels3[n3],\n",
    "        'three_way_ambiguous':   tv + am_relaxed + fr_relaxed,\n",
    "        'three_way_t3':          labels3[min(n3_t3, 3)],\n",
    "\n",
    "        # Four-way concordance (sub-score sums)\n",
    "        'four_way':              struct_s + ext_s,\n",
    "        'four_way_ambiguous_as_pathogenic_ddg_1_threshold': struct_r + ext_r,\n",
    "        'four_way_t3':           struct_s_t3 + ext_s,\n",
    "        'four_way_t3_ambiguous': struct_r_t3 + ext_r,\n",
    "    })\n",
    "\n",
    "\n",
    "conc = df.apply(concordance_v52, axis=1)\n",
    "for c in conc.columns:\n",
    "    df[c] = conc[c]\n",
    "\n",
    "print(f\"\\n\u2713 Concordance computed (v5.2 \u2014 fixed DDG + sub-scores)\")\n",
    "\n",
    "# --- Summary stats ---\n",
    "print(\"\\nSub-score distributions:\")\n",
    "for col in ['structure_strict','structure_relaxed','external_strict','external_relaxed']:\n",
    "    print(f\"  {col}: {df[col].value_counts().sort_index().to_dict()}\")\n",
    "\n",
    "print(f\"\\nFour-way strict:\")\n",
    "print(df['four_way'].value_counts().sort_index().to_string())\n",
    "print(f\"\\nFour-way relaxed:\")\n",
    "print(df['four_way_ambiguous_as_pathogenic_ddg_1_threshold'].value_counts().sort_index().to_string())\n",
    "\n",
    "# --- DDG gating impact ---\n",
    "_has_ddg = df['ddg_monomer'].notna() | df['ddg_multimer_max'].notna() | df['ddg_multimer_min'].notna()\n",
    "_low = _has_ddg & (df['ddg_confidence'] == 'low')\n",
    "_mod = _has_ddg & (df['ddg_confidence'] == 'moderate')\n",
    "_hi  = _has_ddg & (df['ddg_confidence'] == 'high')\n",
    "print(f\"\\nDDG gating impact:\")\n",
    "print(f\"  Variants with DDG data: {_has_ddg.sum()}\")\n",
    "print(f\"  confidence=low  (excluded from all concordance): {_low.sum()}\")\n",
    "print(f\"  confidence=moderate (relaxed only):              {_mod.sum()}\")\n",
    "print(f\"  confidence=high (strict + relaxed):              {_hi.sum()}\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# =============================================================================\n",
    "# CELL 9: SAVE RESULTS (v5.2)\n",
    "# =============================================================================\n",
    "\n",
    "id_cols = ['gene','position','ref_aa','alt_aa']\n",
    "grantham_cols = ['grantham_distance','grantham_class','substitution_severity','property_changes']\n",
    "mono_cols = ['monomer_plddt','monomer_plddt_category','monomer_n_contacts',\n",
    "             'monomer_contact_category','monomer_aa','monomer_accessibility',\n",
    "             'monomer_burial','monomer_secondary_structure','monomer_contact_disruption']\n",
    "summary_cols = ['n_multimer_complexes','multimer_partners']\n",
    "\n",
    "partner_order = ['actin','actb','dvl2','cdh2_truncated','ctnnb1','cdh2_cyto','actb_no_bind','rock2',\n",
    "                 'shroom3','gli3','kpna1','kpna6','mdfi','tcf7l1','zic3']\n",
    "per_complex_cols = []\n",
    "for pl in partner_order:\n",
    "    for suffix in ['_plddt','_n_contacts','_inter_contacts','_is_interface',\n",
    "                   '_accessibility','_burial','_sec_struct','_disruption']:\n",
    "        col = f\"multi_{pl}{suffix}\"\n",
    "        if col in df.columns: per_complex_cols.append(col)\n",
    "\n",
    "summary2 = ['is_interface_any','interface_partners','n_interface_partners',\n",
    "            'multimer_plddt_max','multimer_plddt_avg','multimer_contacts_max',\n",
    "            'multimer_contacts_avg','multimer_disruption_max','multimer_disruption_avg']\n",
    "\n",
    "# v5 scoring columns\n",
    "score_cols_v5 = ['contact_disruption','total_contacts','inter_contacts_sum',\n",
    "                 'best_burial','best_burial_source',\n",
    "                 'final_score','confidence','best_plddt','score_evidence','tier']\n",
    "\n",
    "am_cols = ['AlphaMissense','AlphaMissense_pathogenicity','franklin','integrated_class']\n",
    "ddg_cols = ['ddg_monomer','ddg_category','ddg_category_raw','ddg_confidence',\n",
    "            'pathogenic_mechanism','evidence_summary',\n",
    "            'ddg_multimer_max','ddg_multimer_min','ddg_multimer_mean',\n",
    "            'n_complexes_tested','partners_tested']\n",
    "mech_cols = ['final_mechanism']\n",
    "\n",
    "# v5.2: sub-scores before concordance totals\n",
    "subscore_cols = ['structure_strict','structure_relaxed',\n",
    "                 'structure_strict_t3','structure_relaxed_t3',\n",
    "                 'external_strict','external_relaxed']\n",
    "conc_cols = ['three_way','three_way_ambiguous','three_way_t3',\n",
    "             'four_way','four_way_ambiguous_as_pathogenic_ddg_1_threshold',\n",
    "             'four_way_t3','four_way_t3_ambiguous']\n",
    "\n",
    "ordered = (id_cols + grantham_cols + mono_cols + summary_cols + per_complex_cols +\n",
    "           summary2 + score_cols_v5 + am_cols + ddg_cols + mech_cols +\n",
    "           subscore_cols + conc_cols)\n",
    "remaining = [c for c in df.columns if c not in ordered]\n",
    "final_cols = [c for c in ordered if c in df.columns] + remaining\n",
    "df_out = df[final_cols]\n",
    "\n",
    "out = RESULTS_DIR / \"variant_comprehensive_v5_2.csv\"\n",
    "df_out.to_csv(out, index=False)\n",
    "\n",
    "hp = df_out[df_out['tier'].str.contains('Tier 1|Tier 2', regex=True)]\n",
    "hp.to_csv(RESULTS_DIR / \"high_priority_variants_v5_2.csv\", index=False)\n",
    "\n",
    "scols = ['gene','position','ref_aa','alt_aa','contact_disruption','total_contacts',\n",
    "         'inter_contacts_sum','best_burial','best_burial_source',\n",
    "         'interface_partners','best_plddt','score_evidence','tier',\n",
    "         'ddg_monomer','ddg_category','ddg_confidence',\n",
    "         'ddg_multimer_max','ddg_multimer_min',\n",
    "         'evidence_summary','final_mechanism','AlphaMissense','integrated_class','franklin',\n",
    "         'structure_strict','structure_relaxed','external_strict','external_relaxed',\n",
    "         'three_way','three_way_ambiguous',\n",
    "         'four_way','four_way_ambiguous_as_pathogenic_ddg_1_threshold',\n",
    "         'three_way_t3','four_way_t3','four_way_t3_ambiguous']\n",
    "df_out[[c for c in scols if c in df_out.columns]].to_csv(\n",
    "    RESULTS_DIR / \"variant_pipeline_results_summary_v5_2.csv\", index=False)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PIPELINE v5.2 COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total variants: {len(df_out)}\")\n",
    "print(f\"Total columns:  {len(final_cols)}\")\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION: Concordance spot-checks (v5.2)\n",
    "# =============================================================================\n",
    "print(\"\\n=== CONCORDANCE VALIDATION (v5.2) ===\")\n",
    "\n",
    "def validate_concordance(df_out, gene, pos, expected):\n",
    "    \"\"\"Validate concordance values for a specific variant.\"\"\"\n",
    "    row = df_out[(df_out['gene']==gene)&(df_out['position']==pos)]\n",
    "    if len(row) == 0:\n",
    "        print(f\"\\n  \u26a0 {gene} pos {pos} not found\")\n",
    "        return\n",
    "    r = row.iloc[0]\n",
    "    label = f\"{r['ref_aa']}{pos}{r['alt_aa']}\"\n",
    "    print(f\"\\n  {gene} {label}:\")\n",
    "    all_pass = True\n",
    "    for col, exp_val in expected.items():\n",
    "        got = r.get(col)\n",
    "        try:\n",
    "            match = int(got) == int(exp_val) if isinstance(exp_val, int) else str(got) == str(exp_val)\n",
    "        except:\n",
    "            match = str(got) == str(exp_val)\n",
    "        sym = '\u2713' if match else '\u2717'\n",
    "        if not match: all_pass = False\n",
    "        print(f\"    {sym} {col:45s} got={str(got):>5s}  expected={str(exp_val):>5s}\")\n",
    "    return all_pass\n",
    "\n",
    "# ZIC3 C297F: T1, mono=8.54 (high conf) \u2192 struct=2, AM=path, FR=VUS(high) \u2192 ext=2, 4way=4\n",
    "validate_concordance(df_out, 'zic3', 297, {\n",
    "    'structure_strict': 2, 'structure_relaxed': 2,\n",
    "    'external_strict': 2, 'external_relaxed': 2,\n",
    "    'four_way': 4, 'four_way_ambiguous_as_pathogenic_ddg_1_threshold': 4,\n",
    "})\n",
    "\n",
    "# ZIC3 K405E: T2, mono=-0.15 but multi_max=-4.32 (high conf) \u2192 strict DDG=1 (|4.32|>=2)\n",
    "validate_concordance(df_out, 'zic3', 405, {\n",
    "    'structure_strict': 2, 'structure_relaxed': 2,\n",
    "    'external_strict': 2, 'external_relaxed': 2,\n",
    "    'four_way': 4, 'four_way_ambiguous_as_pathogenic_ddg_1_threshold': 4,\n",
    "})\n",
    "\n",
    "# ZIC3 H318N: T1, mono=0.20, multi_max=-1.71 (high conf) \u2192 strict DDG=0 (|1.71|<2), relax DDG=1\n",
    "validate_concordance(df_out, 'zic3', 318, {\n",
    "    'structure_strict': 1, 'structure_relaxed': 2,\n",
    "    'external_strict': 2, 'external_relaxed': 2,\n",
    "    'four_way': 3, 'four_way_ambiguous_as_pathogenic_ddg_1_threshold': 4,\n",
    "})\n",
    "\n",
    "# SHROOM3 H161Q: T2, mono=-0.15, multi_min=-4.67 (moderate conf) \u2192 strict DDG=0, relax DDG=1\n",
    "validate_concordance(df_out, 'shroom3', 161, {\n",
    "    'structure_strict': 1, 'structure_relaxed': 2,\n",
    "    'external_strict': 0, 'external_relaxed': 2,\n",
    "    'four_way': 1, 'four_way_ambiguous_as_pathogenic_ddg_1_threshold': 4,\n",
    "})\n",
    "\n",
    "# GLI3 P103S: T4, mono=-1.47 (LOW conf) \u2192 DDG=0 regardless, FR=VUS(mid) \u2192 ext_r=1\n",
    "validate_concordance(df_out, 'gli3', 103, {\n",
    "    'structure_strict': 0, 'structure_relaxed': 0,\n",
    "    'external_strict': 0, 'external_relaxed': 1,\n",
    "    'four_way': 0, 'four_way_ambiguous_as_pathogenic_ddg_1_threshold': 1,\n",
    "})\n",
    "\n",
    "# =============================================================================\n",
    "# VALIDATION: Manual score checks against v5 formula (unchanged from v5.1)\n",
    "# =============================================================================\n",
    "print(\"\\n=== SCORING VALIDATION (v5 formula) ===\")\n",
    "\n",
    "def validate_variant(df_out, gene, pos, expected_checks):\n",
    "    \"\"\"Validate a variant's scoring against expected values.\"\"\"\n",
    "    row = df_out[(df_out['gene']==gene)&(df_out['position']==pos)]\n",
    "    if len(row) == 0:\n",
    "        print(f\"\\n  \u26a0 {gene} pos {pos} not found\")\n",
    "        return\n",
    "    r = row.iloc[0]\n",
    "    label = f\"{r['ref_aa']}{pos}{r['alt_aa']}\"\n",
    "    print(f\"\\n  {gene} {label}:\")\n",
    "    for name, col, expected in expected_checks:\n",
    "        got = r.get(col)\n",
    "        got_s = str(got)[:35] if got is not None else 'None'\n",
    "        if expected is None:\n",
    "            match = '\u00b7'\n",
    "        elif isinstance(expected, str):\n",
    "            match = '\u2713' if expected in got_s else '\u2717'\n",
    "        elif isinstance(expected, (int, float)):\n",
    "            try:\n",
    "                match = '\u2713' if abs(float(got) - expected) < 0.15 else '\u2717'\n",
    "            except:\n",
    "                match = '\u2717'\n",
    "        else:\n",
    "            match = '\u2713' if str(expected) in got_s else '\u2717'\n",
    "        print(f\"    {match} {name:30s}  got={got_s:>20s}  expected={str(expected):>15s}\")\n",
    "\n",
    "validate_variant(df_out, 'shroom3', 35, [\n",
    "    ('final_score',  'final_score',  None),\n",
    "    ('tier',         'tier',         'Tier 1'),\n",
    "    ('four_way',     'four_way',     None),\n",
    "])\n",
    "\n",
    "validate_variant(df_out, 'zic3', 350, [\n",
    "    ('final_score',  'final_score',  None),\n",
    "    ('tier',         'tier',         None),\n",
    "    ('four_way',     'four_way',     None),\n",
    "])\n",
    "\n",
    "zero_plddt = df_out['monomer_plddt'].isna().sum()\n",
    "print(f\"\\nMonomer pLDDT missing: {zero_plddt}/{len(df_out)}\")\n",
    "\n",
    "print(f\"\\nSaved to {RESULTS_DIR}/\")\n",
    "print(\"  variant_comprehensive_v5_2.csv\")\n",
    "print(\"  high_priority_variants_v5_2.csv\")\n",
    "print(\"  variant_pipeline_results_summary_v5_2.csv\")\n"
   ],
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}